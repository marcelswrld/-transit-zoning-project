
# -*- coding: utf-8 -*-
"""Combined.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mLHNGl1ZIFT5VxOiS_DksYi_hs7sB5ht
"""

import partridge as ptg
import pandas as pd
import numpy as np
import geopandas as gpd
from shapely.geometry import Point
from shapely.strtree import STRtree
from datetime import datetime
import os
import logging 

logging.basicConfig(level=logging.INFO)

def clean_stop_name(stop_name):
    """Removes EB, WB, NB, SB from stop names to normalize stop identification."""
    return stop_name.replace(" EB", "").replace(" WB", "").replace(" NB", "").replace(" SB", "").strip()

def clean_route_name(route_name):
    """Removes 'to Eastbound' or 'to Westbound' to normalize route filtering."""
    if isinstance(route_name, str):  # Check if it's a string
        return route_name.replace("to Eastbound", "").replace("to Westbound", "").strip()
    return route_name  # Return the original value if it's not a string (e.g., NaN or float)


"""![flowchart](https://i.postimg.cc/Dy3gLM6f/image.png)

# Place file paths here

Replace these file paths with the ones you are using. Each file path should be a zipped or unzipped GTFS file.
"""
# Replace the file paths with your actual GTFS data
gtfs_path_2024 = [
    r"C:\Users\Admin\Downloads\BART_Oct2024-20250211T180718Z-001.zip",
    r"C:\Users\Admin\Downloads\SFBayFerry_Oct2024-20250211T180832Z-001.zip",
    r"C:\Users\Admin\Downloads\VTA_Oct2024-20250211T180828Z-001.zip",
    r"C:\Users\Admin\Downloads\MTS_Oct2024-20250211T180803Z-001.zip",
    r"C:\Users\Admin\Downloads\NCTD_Oct2024-20250211T180813Z-001.zip",
    r"C:\Users\Admin\Downloads\OCTA_Oct2024-20250211T180806Z-001.zip",
    r"C:\Users\Admin\Downloads\Sac_Oct2024-20250211T180809Z-001.zip",
    r"C:\Users\Admin\Downloads\LAMetro_Oct2024-20250211T180746Z-001.zip",
    r"C:\Users\Admin\Downloads\MUNI_Oct2024-20250211T180756Z-001.zip",
    r"C:\Users\Admin\Downloads\LAMetroRail_Oct2024-20250211T180752Z-001.zip",
    r"C:\Users\Admin\Downloads\LADOT_Oct2024-20250211T180744Z-001.zip",
    r"C:\Users\Admin\Downloads\CC_Oct2024-20250211T180738Z-001.zip",
    r"C:\Users\Admin\Downloads\BBB_Oct2024-20250211T180731Z-001.zip",
    r"C:\Users\Admin\Downloads\ACT_Oct2024-20250211T180726Z-001.zip"
]

"""# Maximal/Minimal Toggle

If "maximal" is set to True, the maximal interpretation will be automatically applied throughout the notebook. If it is set to False, the minimal interpretation will be applied.
"""

maximal = True

"""# Step #1: Loading GTFS

This step takes multiple GTFS files and combines them into one dataset.
"""

class GTFSFeed:
    """
    A custom GTFS object for handling concatenated GTFS tables.
    """
    def __init__(self, routes, trips, stop_times, stops):
        self.routes = routes
        self.trips = trips
        self.stop_times = stop_times
        self.stops = stops

def load_and_combine_gtfs(gtfs_paths):
    """
    Loads multiple GTFS files and combine them into a single object.
    """
    # Initialize empty DataFrames for each GTFS table
    combined_routes = pd.DataFrame()
    combined_trips = pd.DataFrame()
    combined_stop_times = pd.DataFrame()
    combined_stops = pd.DataFrame()

    # Loop through each GTFS file and append data
    for path in gtfs_paths:
        prefix = path.stem[:10] if hasattr(path, 'stem') else os.path.splitext(os.path.basename(path))[0]
        feed = ptg.load_feed(path)

    # Clean stop and route names
        feed.stops['cleaned_stop_name'] = feed.stops['stop_name'].apply(clean_stop_name)
        feed.routes['cleaned_route_name'] = feed.routes['route_long_name'].apply(clean_route_name)

       # Make sure ID columns are strings before applying prefixes
        feed.routes['route_id'] = feed.routes['route_id'].astype(str)
        feed.trips['route_id'] = feed.trips['route_id'].astype(str)
        feed.trips['trip_id'] = feed.trips['trip_id'].astype(str)
        feed.stop_times['trip_id'] = feed.stop_times['trip_id'].astype(str)
        feed.stop_times['stop_id'] = feed.stop_times['stop_id'].astype(str)
        feed.stops['stop_id'] = feed.stops['stop_id'].astype(str)

        # Apply prefix to IDs (otherwise routes with same numbers will get mixed up)
        feed.routes['prefixed_route_id'] = prefix + "_" + feed.routes['route_id']
        feed.trips['prefixed_route_id'] = prefix + "_" + feed.trips['route_id']
        feed.trips['prefixed_trip_id'] = prefix + "_" + feed.trips['trip_id']
        feed.stop_times['prefixed_trip_id'] = prefix + "_" + feed.stop_times['trip_id']
        feed.stop_times['prefixed_stop_id'] = prefix + "_" + feed.stop_times['stop_id']
        feed.stops['prefixed_stop_id'] = prefix + "_" + feed.stops['stop_id']

        combined_routes = pd.concat([combined_routes, feed.routes], ignore_index=True)
        combined_trips = pd.concat([combined_trips, feed.trips], ignore_index=True)
        combined_stop_times = pd.concat([combined_stop_times, feed.stop_times], ignore_index=True)
        combined_stops = pd.concat([combined_stops, feed.stops], ignore_index=True)

    # Return a GTFSFeed object
    return GTFSFeed(
        routes=combined_routes,
        trips=combined_trips,
        stop_times=combined_stop_times,
        stops=combined_stops
    )

step1 = load_and_combine_gtfs(gtfs_path_2024)

"""# Step #2: Rail, ferry, BRT stops

This step identifies transit stops that serve rail, light rail, subway, ferry, or some BRT routes. These routes-except for BRT-have their own designation
in GTFS files and all count for HQ transit stops. BRT routes are selected by name.
"""

def rail_ferry_brt(feed, mode = maximal):
    """
    Rail, light rail, and ferry stops from GTFS data, including specific LA BRT lines.

    Params:
        feed: Partridge GTFS feed object created above.

    Returns:
        GeoDataFrame with rail, light rail, and ferry stops, and stops for specified LA BRT lines.
    """
    # Define route types for rail, light rail, and ferry services
    if not mode:
        rail_route_types = [0, 1, 5, 7]  # Light rail, subway, cable car, funicular

    else:
        rail_route_types = [0, 1, 2, 5, 7]  # Light rail, subway, intercity rail, cable car, funicular

    ferry_route_type = 4  # Ferry only

    # Filter routes for rail, light rail, and ferry
    rail_ferry_routes = feed.routes[
        feed.routes['route_type'].isin(rail_route_types + [ferry_route_type])
    ]

    # Include specific LA + SF BRT lines by route_long_name
    la_brt_routes = feed.routes[
        feed.routes['route_long_name'].isin(["Metro G Line 901", "Metro J Line 910/950", "GEARY RAPID"])
    ]

    # Combine rail, ferry, and BRT routes
    all_relevant_routes = pd.concat([rail_ferry_routes, la_brt_routes])

    # Ensure 'route_id' matches between routes and trips
    relevant_trips = feed.trips[
        feed.trips['prefixed_route_id'].isin(all_relevant_routes['prefixed_route_id'])
    ]

    # Ensure 'trip_id' matches between trips and stop_times
    relevant_stop_times = feed.stop_times[
        feed.stop_times['prefixed_trip_id'].isin(relevant_trips['prefixed_trip_id'])
    ]

    # Ensure 'stop_id' matches between stop_times and stops
    relevant_stops = feed.stops[
        feed.stops['prefixed_stop_id'].isin(relevant_stop_times['prefixed_stop_id'])
        ]

    # Create GeoDataFrame for spatial analysis
    rail_ferry_brt_gdf = gpd.GeoDataFrame(
        relevant_stops,
        geometry=gpd.points_from_xy(relevant_stops.stop_lon, relevant_stops.stop_lat),
        crs="EPSG:4326"  # Best projection for the area
    )

    print("Rail, ferry, and BRT stops extracted.")
    return rail_ferry_brt_gdf

step2 = rail_ferry_brt(step1)

"""# Step #3: Bus stops with peak hours

This step identifies bus stops with frequent service during peak hours (morning and afternoon). In the maximal definition, a stop qualifies
if it has, from one route, at least one hour-long period with 3 arrivals throughout the morning and afternoon peak hours. This hour-long period need not start at the beginning of an hour; for example, it could stretch from 7:30-8:30AM. In the minimal definition, a stop qualifies if it has, from one route, at least 9 buses arriving during the morning peak AND 12 buses in the afternoon peak. The morning peak is 6-9AM and the afternoon peak is 3-7PM.
"""

def bus_stops_peak_hours(feed, mode=maximal):
    """
    Returns:
        GDF containing bus stops with at least 21 total arrivals during peak hours for at least one route.
        Ensures minimal stops are included in maximal definitions.
    """

    if not mode:
        am_peak_start = pd.to_datetime('06:00:00').time()
        am_peak_end = pd.to_datetime('08:59:59').time()
        pm_peak_start = pd.to_datetime('15:00:00').time()
        pm_peak_end = pd.to_datetime('18:59:59').time()
    else:
        am_peak_start = pd.to_datetime('00:00:00').time()
        am_peak_end = pd.to_datetime('11:59:59').time()
        pm_peak_start = pd.to_datetime('12:00:00').time()
        pm_peak_end = pd.to_datetime('23:59:59').time()

    # Include minimal stops in maximal definitions

    qualifying_stops = set()
    if mode:
        minimal_stops = bus_stops_peak_hours(feed, mode=False)
        qualifying_stops = qualifying_stops.union(set(minimal_stops['prefixed_stop_id']))


    # Only buses
    bus_routes = feed.routes[feed.routes['route_type'] == 3]  # Bus route_type = 3

    # Use `route_id` if `prefixed_route_id` is missing
    route_key = 'prefixed_route_id' if 'prefixed_route_id' in bus_routes.columns else 'route_id'
    trip_key = 'prefixed_trip_id' if 'prefixed_trip_id' in feed.trips.columns else 'trip_id'

    # Bus trips
    bus_trips = feed.trips[feed.trips[route_key].isin(bus_routes[route_key])]

    # Merge trip and route data into stop_times
    stop_times_enriched = feed.stop_times.merge(
        bus_trips[[trip_key, route_key]],
        left_on=trip_key,
        right_on=trip_key,
        how="inner"
    )

    # Convert arrival times from seconds to datetime.time
    stop_times_enriched['arrival_time'] = pd.to_datetime(stop_times_enriched['arrival_time'], unit='s').dt.time

    # Filter stop times for AM and PM peak hours (only arrivals)
    am_peak_stop_times = stop_times_enriched[
        (stop_times_enriched['arrival_time'] >= am_peak_start) & (stop_times_enriched['arrival_time'] <= am_peak_end)
    ]

    pm_peak_stop_times = stop_times_enriched[
        (stop_times_enriched['arrival_time'] >= pm_peak_start) & (stop_times_enriched['arrival_time'] <= pm_peak_end)
    ]

    # Group by stop_id and route_key for AM and PM peak times
    am_route_stop_counts = am_peak_stop_times.groupby(['prefixed_stop_id', route_key]).size()
    pm_route_stop_counts = pm_peak_stop_times.groupby(['prefixed_stop_id', route_key]).size()

    # Filter for stops meeting thresholds for at least one route
    qualifying_am_stops = am_route_stop_counts[am_route_stop_counts >= 9].reset_index()['prefixed_stop_id']
    qualifying_pm_stops = pm_route_stop_counts[pm_route_stop_counts >= 12].reset_index()['prefixed_stop_id']

    # Identify stops that meet both AM and PM criteria for at least one route
    qualifying_stops = set(qualifying_am_stops).intersection(set(qualifying_pm_stops))

    # Include minimal stops in maximal definition
    if mode:
        logging.info("Including minimal stops in maximal definition.")
        minimal_stops = bus_stops_peak_hours(feed, mode=False)
        qualifying_stops = qualifying_stops.union(set(minimal_stops['prefixed_stop_id']))

    # Extract corresponding stops from the stops table
    bus_peak_stops = feed.stops[feed.stops['prefixed_stop_id'].isin(qualifying_stops)]

    # Convert to GeoDataFrame for spatial analysis
    bus_peak_gdf = gpd.GeoDataFrame(
        bus_peak_stops,
        geometry=gpd.points_from_xy(bus_peak_stops.stop_lon, bus_peak_stops.stop_lat),
        crs="EPSG:4326"
    )

    print(f"Identified {len(bus_peak_gdf)} high-frequency bus stops meeting route-specific thresholds.")
    return bus_peak_gdf

step3 = bus_stops_peak_hours(step1)

"""# Step #4: Bus stop intersections

This step finds bus stops that are near each other (within 150 or 500 feet) but serve different bus routes. This prevents stops on the same route
from intersecting, which Cal-ITP believed was against the intent of the law.
"""

def identify_bus_stop_intersections(feed, bus_peak_gdf, mode=maximal):
    """
    Identifies intersecting bus stops based on a distance threshold, excluding stops on the same route.
    """
    # Reproject to UTM Zone 11N for accurate distance calculations
    bus_peak_gdf = bus_peak_gdf.to_crs(epsg=32611)

    if not mode:
        distance_threshold = 150 * 0.3048  # Convert feet to meters
    else:
        distance_threshold = 500 * 0.3048  # Convert feet to meters

    
    # Join route and direction information to bus stops by trip_id
    trip_route_data = feed.trips[['trip_id', 'route_id', 'direction_id']]
    stop_trip_route_gdf = (
        bus_peak_gdf.merge(feed.stop_times[['stop_id', 'trip_id']], on='stop_id')
        .merge(trip_route_data, on='trip_id')
        .merge(feed.routes[['route_id', 'cleaned_route_name']], on='route_id')  # Use cleaned route names
        .drop_duplicates(subset=['stop_id', 'route_id', 'direction_id'])
    )

    #✅ Ensure `stop_trip_route_gdf` is defined BEFORE calling `.copy()`
    if stop_trip_route_gdf.empty:
        print("⚠️ No valid stop-trip-route data found! Check data processing.")
        return gpd.GeoDataFrame()

    # Create a buffered version of stops
    buffered = stop_trip_route_gdf.copy()
    buffered["geometry"] = buffered.geometry.buffer(distance_threshold)

    # Use spatial join to find intersections
    intersections = gpd.sjoin(stop_trip_route_gdf, buffered, how="inner", predicate="intersects")

    # Filter valid intersections (avoid same-route, same-direction stops)
    valid_intersections = intersections[
        (intersections['cleaned_route_name_left'] != intersections['cleaned_route_name_right']) &
        (intersections['direction_id_left'] != intersections['direction_id_right'])
    ]

    # Convert results into a set for faster lookup
    intersecting_stops = set(valid_intersections['stop_id_left'])

    # Log detected intersections
    for _, row in valid_intersections.iterrows():
        logging.info(
            f"Intersection detected: \n"
            f"- Stop {row['stop_id_left']} ({row['cleaned_route_name_left']})\n"
            f"- Stop {row['stop_id_right']} ({row['cleaned_route_name_right']})\n"
            f"Distance threshold: {distance_threshold}m"
        )

    # Return filtered results
    intersecting_stops_gdf = bus_peak_gdf[bus_peak_gdf['stop_id'].isin(intersecting_stops)]
    print(f"✅ Identified {len(intersecting_stops_gdf)} valid intersections")

    return intersecting_stops_gdf

step4 = identify_bus_stop_intersections(step1, step3)


"""# Step #5: Merge datasets

This step combines the rail/ferry stops and high-quality bus stops (from steps 2 and 4) into one dataset. It then exports this dataset as a shapefile of the stops (without 1/2 mile buffers) to a designated file path.
"""

def merge_transit_stops(rail_ferry_brt, bus_intersections_gdf, mode = maximal,
                        output_path = r"C:\Users\marce\OneDrive\Documents\UCLA ITS\Transit Zoning Project\Transit Job Output"):
    """
    Merges the rail/ferry, intersecting bus, and high-frequency corridor GDFs
    into a single GDF, export
    """
    # Ensure all GeoDataFrames are in the same CRS
    if rail_ferry_brt.crs != bus_intersections_gdf.crs:
        bus_intersections_gdf = bus_intersections_gdf.to_crs(rail_ferry_brt.crs)

    if not mode:
        distance_threshold = 150

    else:
        distance_threshold = 500

    rail_ferry_brt['qualification'] = 'Rail, Ferry, BRT stop'
    bus_intersections_gdf['qualification'] = f'Bus stop with intersection within {distance_threshold} feet'

    # Step 2: Concatenate GeoDataFrames
    all_high_quality_stops_gdf = gpd.GeoDataFrame(
    pd.concat([rail_ferry_brt, bus_intersections_gdf, #corridor_stops_gdf
              ], ignore_index=True)
    )

    # Aggregate qualifications for duplicate stops
    # Group by stop_id, keeping geometry and combining qualifications as a list
    all_high_quality_stops_aggregated = (
    all_high_quality_stops_gdf
    .groupby('stop_id', as_index=False)
    .agg({
        'qualification': lambda x: '; '.join(set(x)),  # Combine unique qualifications as a single string
        'geometry': 'first',  # Keep the first geometry entry for each stop_id
        'stop_lat': 'first',  # Keep the latitude and longitude
        'stop_lon': 'first',
        'stop_name': 'first' # These are the fields each GTFS is guaranteed to have, using others returns errors

    })
    )

    all_high_quality_stops_aggregated_gdf = gpd.GeoDataFrame(
    all_high_quality_stops_aggregated,
    geometry='geometry',
    crs="EPSG:4326")

    # ✅ Shorten column names to 10 characters for Shapefile compatibility
    column_renaming = {
        "stop_id": "stop_id",
        "qualification": "qualificat",  # Truncate to 10 chars
        "geometry": "geometry",
        "stop_lat": "stop_lat",
        "stop_lon": "stop_lon",
        "stop_name": "stop_name",
    }
    all_high_quality_stops_aggregated_gdf = all_high_quality_stops_aggregated_gdf.rename(columns=column_renaming)

    # ✅ Save file using os.path.join()
    output_file = os.path.join(output_path, "file2.shp")

    try:
        all_high_quality_stops_aggregated_gdf.to_file(output_file, driver="ESRI Shapefile")
        print(f"✅ Saved shapefile: {output_file}")

    except Exception as e:
        print(f"❌ Error saving shapefile: {e}")

    return all_high_quality_stops_aggregated_gdf

step5 = merge_transit_stops(step2, step4)

"""# Step #6: Buffer transit stops

This step creates a buffer zone (1/2 mile) around each of the transit stops. These are the high-quality transit areas resulting from
the stops. This step exports the buffer zone to a designated file path.
"""

def buffer_transit_stops(high_quality_stops_gdf, buffer_distance_miles = 0.5,
                         output_path = r"C:\Users\marce\OneDrive\Documents\UCLA ITS\Transit Zoning Project\Transit Job Output"):
    """
    Buffer 1/2 mile around each stop and save to shapefile.
    """
    # Convert buffer distance from miles to meters
    buffer_distance_meters = buffer_distance_miles * 1609.34

    # Use UTM Zone 11N for buffering accuracy (best for buffering, I learned)
    if high_quality_stops_gdf.crs.to_epsg() != 32611:  # UTM Zone 11N
        high_quality_stops_gdf = high_quality_stops_gdf.to_crs(epsg=32611)

    # Apply the buffer and set it as the active geometry
    high_quality_stops_gdf['buffer'] = high_quality_stops_gdf.geometry.buffer(buffer_distance_meters)
    buffered_stops_gdf = high_quality_stops_gdf.set_geometry('buffer').copy()

    # Drop the original geometry to avoid conflicts when saving
    buffered_stops_gdf = buffered_stops_gdf.drop(columns='geometry')

    # Save to file
    # Don't forget to comment out if not saving!
  # ✅ Standardize output path
    output_file = os.path.join(output_path, "file_buffer2.shp")

    try:
        buffered_stops_gdf.to_file(output_file, driver="ESRI Shapefile")
        print(f"✅ Saved shapefile: {output_file}")

    except Exception as e:
        print(f"❌ Error saving shapefile: {e}")

    return buffered_stops_gdf

step6 = buffer_transit_stops(step5)