
# -*- coding: utf-8 -*-
"""Combined.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mLHNGl1ZIFT5VxOiS_DksYi_hs7sB5ht
""" 

import partridge as ptg
import pandas as pd
import numpy as np
import geopandas as gpd
from datetime import datetime
import os
import logging 


logging.basicConfig(level=logging.INFO)

"""![flowchart](https://i.postimg.cc/Dy3gLM6f/image.png)

# Place file paths here

Replace these file paths with the ones you are using. Each file path should be a zipped or unzipped GTFS file.
"""
# Replace the file paths with your actual GTFS data
# gtfs_path_2024 = [
#     r"C:\Users\marce\Downloads\BART_Oct2024-20250211T180718Z-001.zip",
#     r"C:\Users\marce\Downloads\SFBayFerry_Oct2024-20250211T180832Z-001.zip",
#     r"C:\Users\marce\Downloads\VTA_Oct2024-20250211T180828Z-001.zip",
#     r"C:\Users\marce\Downloads\MTS_Oct2024-20250211T180803Z-001.zip",
#     r"C:\Users\marce\Downloads\NCTD_Oct2024-20250211T180813Z-001.zip",
#     r"C:\Users\marce\Downloads\OCTA_Oct2024-20250211T180806Z-001.zip",
#     r"C:\Users\marce\Downloads\Sac_Oct2024-20250211T180809Z-001.zip",
#     r"C:\Users\marce\Downloads\LAMetro_Oct2024-20250211T180746Z-001.zip",
#     r"C:\Users\marce\Downloads\MUNI_Oct2024-20250211T180756Z-001.zip",
#     r"C:\Users\marce\Downloads\LAMetroRail_Oct2024-20250211T180752Z-001.zip",
#     r"C:\Users\marce\Downloads\LADOT_Oct2024-20250211T180744Z-001.zip",
#     r"C:\Users\marce\Downloads\CC_Oct2024-20250211T180738Z-001.zip",
#     r"C:\Users\marce\Downloads\BBB_Oct2024-20250211T180731Z-001.zip",
#     r"C:\Users\marce\Downloads\ACT_Oct2024-20250211T180726Z-001.zip"
# ]

gtfs_path_2024 = [
    r"C:\Users\marce\Downloads\LAMetroRail_Oct2024-20250211T180752Z-001.zip",
    r"C:\Users\marce\Downloads\LAMetroRail_Oct2024-20250211T180752Z-001.zip",
    r"C:\Users\marce\Downloads\LADOT_Oct2024-20250211T180744Z-001.zip",
    r"C:\Users\marce\Downloads\BBB_Oct2024-20250211T180731Z-001.zip"
]
"""# Maximal/Minimal Toggle

If "maximal" is set to True, the maximal interpretation will be automatically applied throughout the notebook. If it is set to False, the minimal interpretation will be applied.
"""

maximal = True  # or 'maximal' when you want to switch modes

"""# Step #1: Loading GTFS

This step takes multiple GTFS files and combines them into one dataset.
"""

class GTFSFeed:
    """A custom GTFS object for handling concatenated GTFS tables."""
    def __init__(self, routes, trips, stop_times, stops, agency, frequencies=None):
        self.routes = routes
        self.trips = trips
        self.stop_times = stop_times
        self.stops = stops       
        self.agency = agency
        self.frequencies = frequencies if frequencies is not None else pd.DataFrame()

def load_and_combine_gtfs(gtfs_paths):
    """Loads multiple GTFS files and combines them into a single object."""
    combined_routes = pd.DataFrame()
    combined_trips = pd.DataFrame()
    combined_stop_times = pd.DataFrame()
    combined_stops = pd.DataFrame()
    combined_agency = pd.DataFrame()
    combined_frequencies = pd.DataFrame()

    for path in gtfs_paths:
        prefix = os.path.splitext(os.path.basename(path))[0][:10]
        print(f"Processing feed: {path}")
        feed = ptg.load_feed(path, view=None)
        
        # Extract DataFrames
        routes = feed.routes.copy()
        trips = feed.trips.copy()
        stop_times = feed.stop_times.copy()
        stops = feed.stops.copy()
        
        # Read agency data - let errors occur naturally as supervisor suggested
        agency = feed.agency.copy() if hasattr(feed, 'agency') else pd.DataFrame({'agency_id': ['1'], 'agency_name': ['Unknown Agency']})
        
        if 'agency_id' in routes.columns:
            routes['agency_id'] = routes['agency_id'].astype(str)
        else:
            routes['agency_id'] = '1'  # Assign the default value if column doesn't exist
        
        # Read frequencies if available - let errors occur naturally
        frequencies = feed.frequencies.copy() if hasattr(feed, 'frequencies') else pd.DataFrame()
        
        # Apply prefixes to IDs consistently
        routes['agency_id'] = routes['agency_id'].astype(str)
        routes['prefixed_agency_id'] = prefix + "_" + routes['agency_id']
        routes['prefixed_route_id'] = prefix + "_" + routes['route_id'].astype(str)
        
        trips['prefixed_route_id'] = prefix + "_" + trips['route_id'].astype(str)
        trips['prefixed_trip_id'] = prefix + "_" + trips['trip_id'].astype(str)
        
        stop_times['prefixed_trip_id'] = prefix + "_" + stop_times['trip_id'].astype(str)
        stop_times['prefixed_stop_id'] = prefix + "_" + stop_times['stop_id'].astype(str)
        
        stops['prefixed_stop_id'] = prefix + "_" + stops['stop_id'].astype(str)
        
        agency['prefixed_agency_id'] = prefix + "_" + agency['agency_id']
        
        if not frequencies.empty and 'trip_id' in frequencies.columns:
            frequencies['prefixed_trip_id'] = prefix + "_" + frequencies['trip_id'].astype(str)
        
        # Merge agency info into routes with a simple one-liner
        routes = routes.merge(agency[['prefixed_agency_id', 'agency_name']], 
                             on='prefixed_agency_id', how='left')
        
        # Combine data
        combined_routes = pd.concat([combined_routes, routes], ignore_index=True)
        combined_trips = pd.concat([combined_trips, trips], ignore_index=True)
        combined_stop_times = pd.concat([combined_stop_times, stop_times], ignore_index=True)
        combined_stops = pd.concat([combined_stops, stops], ignore_index=True)
        combined_agency = pd.concat([combined_agency, agency], ignore_index=True)
        if not frequencies.empty:
            combined_frequencies = pd.concat([combined_frequencies, frequencies], ignore_index=True)

    # Merge frequencies into trips if available
    if not combined_frequencies.empty:
        combined_trips = combined_trips.merge(
            combined_frequencies[['prefixed_trip_id', 'headway_secs']],
            on='prefixed_trip_id', how='left'
        )
    
    return GTFSFeed(
        routes=combined_routes,
        trips=combined_trips,
        stop_times=combined_stop_times,
        stops=combined_stops,
        agency=combined_agency,
        frequencies=combined_frequencies
    )

"""# Step #2: Rail, ferry, BRT stops

This step identifies transit stops that serve rail, light rail, subway, ferry, or some BRT routes. These routes-except for BRT-have their own designation
in GTFS files and all count for HQ transit stops. BRT routes are selected by name.
"""

def rail_ferry_brt(feed, mode='maximal'):
    """Identifies rail, ferry, and BRT stops from GTFS data."""
    # Define route types based on mode
    if mode == 'minimal':
        rail_route_types = [0, 1, 5, 7]  # Tram, Subway, Cable Car, Funicular
    else:
        rail_route_types = [0, 1, 2, 5, 7]  # Including Intercity Rail
    
    ferry_route_type = 4  # Ferry

    # Filter routes for rail and ferry
    rail_ferry_routes = feed.routes[feed.routes['route_type'].isin(rail_route_types + [ferry_route_type])]
    
    # Include specific BRT lines by route_long_name
    brt_lines = ["Metro G Line 901", "Metro J Line 910/950", "GEARY RAPID"]
    brt_routes = feed.routes[feed.routes['route_long_name'].isin(brt_lines)]
    
    # Combine rail, ferry, and BRT routes
    all_routes = pd.concat([rail_ferry_routes, brt_routes], ignore_index=True)
    
    # Get stops for these routes
    relevant_trips = feed.trips[feed.trips['prefixed_route_id'].isin(all_routes['prefixed_route_id'])]
    relevant_stop_times = feed.stop_times[feed.stop_times['prefixed_trip_id'].isin(relevant_trips['prefixed_trip_id'])]
    relevant_stops = feed.stops[feed.stops['prefixed_stop_id'].isin(relevant_stop_times['prefixed_stop_id'])]
    
    # Create GeoDataFrame
    rail_ferry_brt_gdf = gpd.GeoDataFrame(
        relevant_stops,
        geometry=gpd.points_from_xy(relevant_stops['stop_lon'], relevant_stops['stop_lat']),
        crs="EPSG:4326"
    )
    
    print(f"Extracted {len(rail_ferry_brt_gdf)} rail, ferry, and BRT stops")
    return rail_ferry_brt_gdf

"""# Step #3: Bus stops with peak hours

This step identifies bus stops with frequent service during peak hours (morning and afternoon). In the maximal definition, a stop qualifies
if it has, from one route, at least one hour-long period with 3 arrivals throughout the morning and afternoon peak hours. This hour-long period need not start at the beginning of an hour; for example, it could stretch from 7:30-8:30AM. In the minimal definition, a stop qualifies if it has, from one route, at least 9 buses arriving during the morning peak AND 12 buses in the afternoon peak. The morning peak is 6-9AM and the afternoon peak is 3-7PM.
"""

def bus_stops_peak_hours(feed, mode='maximal'):
    """Identifies bus stops with frequent service during peak hours."""
    # Only select bus routes
    bus_routes = feed.routes[feed.routes['route_type'] == 3]  # Bus route_type = 3
    bus_trips = feed.trips[feed.trips['prefixed_route_id'].isin(bus_routes['prefixed_route_id'])]
    
    # Enrich stop_times with route information
    stop_times = feed.stop_times.merge(
        bus_trips[['prefixed_trip_id', 'prefixed_route_id']],
        on='prefixed_trip_id', how='inner'
    )
   
    # Convert arrival times to datetime - let errors occur naturally as supervisor suggested
    # Use pd.to_datetime with proper format handling
    stop_times['time'] = pd.to_datetime(stop_times['arrival_time'], unit='s')
    
    # Define peak periods based on mode
    if mode == 'minimal':
        am_start = pd.to_datetime('1970-01-01 06:00:00')
        am_end = pd.to_datetime('1970-01-01 09:00:00')
        pm_start = pd.to_datetime('1970-01-01 15:00:00')
        pm_end = pd.to_datetime('1970-01-01 19:00:00')
        
        # Filter for AM and PM peak hours
        am_peak = stop_times[(stop_times['time'] >= am_start) & (stop_times['time'] < am_end)]
        pm_peak = stop_times[(stop_times['time'] >= pm_start) & (stop_times['time'] < pm_end)]
        
        # Count trips per stop per route during peaks
        am_counts = am_peak.groupby(['prefixed_stop_id', 'prefixed_route_id']).size().reset_index(name='am_count')
        pm_counts = pm_peak.groupby(['prefixed_stop_id', 'prefixed_route_id']).size().reset_index(name='pm_count')
        
        # Join the counts
        peak_counts = am_counts.merge(pm_counts, on=['prefixed_stop_id', 'prefixed_route_id'], how='outer').fillna(0)
        
        # Filter stops with at least 9 AM trips AND 12 PM trips on any route - storing route information
        qualifying_stops = peak_counts[(peak_counts['am_count'] >= 9) & (peak_counts['pm_count'] >= 12)][['prefixed_stop_id','prefixed_route_id']].drop_duplicates()
        qualifying_stops.set_index('prefixed_stop_id', inplace=True)
    
    else:  # maximal mode
        # Check for 3+ trips per hour in both AM and PM periods
        am_start = pd.to_datetime('1970-01-01 00:00:00')
        am_end = pd.to_datetime('1970-01-01 11:59:59')  # Fixed from 24:00:00
        pm_start = pd.to_datetime('1970-01-01 12:00:00')
        pm_end = pd.to_datetime('1970-01-01 23:59:59')  # Fixed from 24:00:00
        
        # Filter for AM and PM periods
        am_peak = stop_times[(stop_times['time'] >= am_start) & (stop_times['time'] < am_end)]
        pm_peak = stop_times[(stop_times['time'] >= pm_start) & (stop_times['time'] < pm_end)]
        
        # Check for rolling hour windows with 3+ trips
        qualifying_stops_am = set()
        qualifying_stops_pm = set()
        
        # Process AM peak
        for stop_route, group in am_peak.groupby(['prefixed_stop_id', 'prefixed_route_id']):
            times = sorted(group['time'])
            for i, start_time in enumerate(times):
                end_time = start_time + pd.Timedelta(hours=1)
                count = sum((pd.Series(times) >= start_time) & (pd.Series(times) < end_time))
                if count >= 3:
                    qualifying_stops_am.add(stop_route)  # Store both stop_id and route_id
                    break
        
        # Process PM peak
        for stop_route, group in pm_peak.groupby(['prefixed_stop_id', 'prefixed_route_id']):
            times = sorted(group['time'])
            for i, start_time in enumerate(times):
                end_time = start_time + pd.Timedelta(hours=1)
                count = sum((pd.Series(times) >= start_time) & (pd.Series(times) < end_time))
                if count >= 3:
                    qualifying_stops_pm.add(stop_route)  # Store both stop_id and route_id
                    break
        
        # A stop must qualify during both AM and PM periods
        qualifying_stops = pd.DataFrame(qualifying_stops_am.intersection(qualifying_stops_pm), 
                                      columns=['prefixed_stop_id', 'prefixed_route_id']).set_index('prefixed_stop_id')
    
    # Get stop details for qualifying stops - join with stop_lon and stop_lat only
    bus_peak_stops = qualifying_stops.join(feed.stops.set_index('prefixed_stop_id')[['stop_lon','stop_lat', 'stop_name']])
    
    # Create GeoDataFrame
    bus_peak_gdf = gpd.GeoDataFrame(
        bus_peak_stops,
        geometry=gpd.points_from_xy(bus_peak_stops['stop_lon'], bus_peak_stops['stop_lat']),
        crs="EPSG:4326"
    )
    
    print(f"Identified {len(bus_peak_gdf)} high-frequency bus stops")
    return bus_peak_gdf


"""# Step #4: Bus stop intersections

This step finds bus stops that are near each other (within 150 or 500 feet) but serve different bus routes. This prevents stops on the same route
from intersecting, which Cal-ITP believed was against the intent of the law.
"""



"""
    Identifies intersecting bus stops based on a distance threshold, excluding stops on the same route.
    
    Parameters:
    - feed: GTFS feed containing routes, trips, and stop_times
    - bus_peak_gdf: GeoDataFrame with bus stops meeting peak hour criteria
    - mode: 'minimal' or 'maximal' mode
    
    Returns:
    - GeoDataFrame with qualifying intersecting bus stops
    """
def identify_bus_stop_intersections(feed, bus_peak_gdf, mode='maximal'):
    """
    Identifies bus stops that qualify based on the intersection rules.
    
    For minimal definition:
    - Only actual intersections between different routes count
    - No combining parallel routes
    
    For maximal definition:
    - All stops along a route with any intersection count
    - Routes along same road count as intersecting
    """
    print(f"Finding bus stop intersections in {mode} mode")
    
   # Set distance threshold based on mode and convert from feet to meters for UTM projection
    distance_threshold_feet = 150 if mode == 'minimal' else 500  # feet
    distance_threshold_meters = distance_threshold_feet * 0.3048  # Conversion to meters
    
    # Ensure CRS is consistent and bus_peak_gdf is indexed by prefixed_stop_id
    bus_peak_gdf = bus_peak_gdf.to_crs(epsg=32611)  # Project to UTM for accurate distances
    
    # Ensure index is prefixed_stop_id
    if bus_peak_gdf.index.name != 'prefixed_stop_id':
        bus_peak_gdf = bus_peak_gdf.set_index('prefixed_stop_id')
    
    # Add plain English route names
    routenames = feed.routes[['prefixed_route_id','agency_name','route_short_name']].set_index('prefixed_route_id')
    routenames['agency_route'] = routenames.agency_name + ' ' + routenames.route_short_name
    routenames = routenames['agency_route'].to_dict()
    
    # Get unique stops for initial spatial query (to avoid duplicates)
    unique_stops = bus_peak_gdf[~bus_peak_gdf.index.duplicated()]
    
    # Find stops within distance threshold of each other
    intersecting_stops = set()
    intersecting_stop_routes = {}
    
    # Process each stop
    for stop_id1, stop1 in unique_stops.iterrows():
        # Get all routes at this stop
        routes_at_stop1 = set(bus_peak_gdf.loc[[stop_id1], 'prefixed_route_id'])
        
        # Find all stops within threshold distance
        nearby_stop_ids = unique_stops[unique_stops.geometry.distance(stop1.geometry) <= distance_threshold_meters].index
        nearby = bus_peak_gdf.loc[nearby_stop_ids]
        
        # Exclude self
        nearby = nearby[nearby.index != stop_id1]
        
        if len(nearby) == 0:
            continue
            
        # Get routes at nearby stops
        routes_at_nearby_stops = set(bus_peak_gdf.loc[nearby.index, 'prefixed_route_id'])
        
        # Get all routes (at this stop and nearby)
        all_routes = routes_at_stop1.union(routes_at_nearby_stops)
        
        # Add plain English route names
        all_routes_english = [routenames.get(rr, str(rr)) for rr in all_routes]
        
        # Implement minimal vs maximal logic
        if mode == 'maximal':
            # If the stop and nearby stops have >1 route, it qualifies!
            if len(all_routes) > 1:
                intersecting_stops.add(stop_id1)
                intersecting_stop_routes[stop_id1] = all_routes_english
        else:
            # For minimal: exclude routes that stop at the same stop
            # Only count if there are routes at nearby stops that aren't at this stop
            if len(routes_at_nearby_stops.difference(routes_at_stop1)) > 0:
                intersecting_stops.add(stop_id1)
                intersecting_stop_routes[stop_id1] = all_routes_english
    
    # Generate result using supervisor's method
    result = unique_stops.loc[list(intersecting_stops)][['geometry']]
    result = result.join(pd.Series(intersecting_stop_routes).to_frame('prefixed_route_ids'))
    result = result.to_crs(epsg=4326)  # Convert back to WGS84
    
    # Add qualification field
    result['qualification'] = f'Bus stop with intersection within {distance_threshold_feet} feet'
    
    print(f"Found {len(result)} qualifying bus stops with intersections")
    return result.reset_index()


"""# Step #5: Merge datasets

This step combines the rail/ferry stops and high-quality bus stops (from steps 2 and 4) into one dataset. It then exports this dataset as a shapefile of the stops (without 1/2 mile buffers) to a designated file path.
"""
def merge_transit_stops(rail_ferry_brt_gdf, bus_stops_gdf, mode='maximal', output_path='output'):
    """Merges rail/ferry/BRT stops with high-frequency bus stops."""
    # Ensure CRS is consistent
    if rail_ferry_brt_gdf.crs is None:
        rail_ferry_brt_gdf.set_crs(epsg=4326, inplace=True)
    
    if bus_stops_gdf is not None and not bus_stops_gdf.empty:
        if bus_stops_gdf.crs is None:
            bus_stops_gdf.set_crs(epsg=4326, inplace=True)
        if rail_ferry_brt_gdf.crs != bus_stops_gdf.crs:
            bus_stops_gdf = bus_stops_gdf.to_crs(rail_ferry_brt_gdf.crs)
    
    # Assign qualification labels
    rail_ferry_brt_gdf['qualification'] = 'Rail, Ferry, BRT stop'
    
    if bus_stops_gdf is None or bus_stops_gdf.empty:
        print("No qualifying bus stops found. Using only rail/ferry/BRT stops.")
        combined_gdf = rail_ferry_brt_gdf.copy()
    else:
        bus_stops_gdf['qualification'] = 'High-Frequency Bus Stop'
        combined_gdf = pd.concat([rail_ferry_brt_gdf, bus_stops_gdf])
    
    # Group by stop_id to handle duplicates
    agg_funcs = {
        'qualification': lambda x: '; '.join(set(x.dropna())),
        'geometry': 'first',
        'stop_lat': 'first',
        'stop_lon': 'first',
        'stop_name': 'first',
        'agency_name': lambda x: '; '.join(set(x.dropna().astype(str)))
    }
    
    all_stops_aggregated = combined_gdf.groupby('prefixed_stop_id').agg(agg_funcs).reset_index()
    
    # Convert to GeoDataFrame
    result = gpd.GeoDataFrame(all_stops_aggregated, geometry='geometry', crs=rail_ferry_brt_gdf.crs)
    
    # Rename columns for shapefile compatibility if needed
    result.rename(columns={
        'prefixed_stop_id': 'stop_id',
        'qualification': 'qualify',
        'agency_name': 'agency'
    }, inplace=True)
    
    # Create output directory if it doesn't exist
    os.makedirs(output_path, exist_ok=True)
    
    # Save both GeoPackage and Shapefile with mode in filename
    result.to_file(os.path.join(output_path, f"high_quality_stops_{mode}.gpkg"), driver="GPKG")
    result.to_file(os.path.join(output_path, f"high_quality_stops_{mode}.shp"), driver="ESRI Shapefile")
    
    print(f"Saved merged stops to {output_path}")
    return result
"""# Step #6: Buffer transit stops

This step creates a buffer zone (1/2 mile) around each of the transit stops. These are the high-quality transit areas resulting from
the stops. This step exports the buffer zone to a designated file path.
"""
def buffer_transit_stops(high_quality_stops_gdf, buffer_distance_miles=0.5, mode='maximal', output_path='output'):
    """Creates a buffer zone around transit stops."""
    # Convert buffer distance to meters
    buffer_distance_meters = buffer_distance_miles * 1609.34
    
    # Project to UTM for accurate buffering
    projected_gdf = high_quality_stops_gdf.to_crs(epsg=32611)
    
    # Apply buffer directly to geometry (simplified approach)
    projected_gdf.geometry = projected_gdf.geometry.buffer(buffer_distance_meters)
    
    # Create output directory if it doesn't exist
    os.makedirs(output_path, exist_ok=True)
    
    # Save both GeoPackage and Shapefile with mode in filename
    projected_gdf.to_file(os.path.join(output_path, f"buffered_stops_{mode}.gpkg"), driver="GPKG")
    projected_gdf.to_file(os.path.join(output_path, f"buffered_stops_{mode}.shp"), driver="ESRI Shapefile")
    
    print(f"Saved buffered stops to {output_path}")
    return projected_gdf


def run_transit_zoning_pipeline(gtfs_paths, output_base_path):
    """Runs the complete pipeline for both minimal and maximal definitions."""
    print("Starting Transit Zoning Pipeline")
    
    # Create output directories
    output_path_max = os.path.join(output_base_path, "Maximal")
    output_path_min = os.path.join(output_base_path, "Minimal")
    os.makedirs(output_path_max, exist_ok=True)
    os.makedirs(output_path_min, exist_ok=True)
    
    # Step 1: Load and combine GTFS data
    print("Step 1: Loading GTFS data")
    feed = load_and_combine_gtfs(gtfs_paths)
    
    # Process maximal definition
    print("Processing maximal definition")
    rail_ferry_brt_max = rail_ferry_brt(feed, mode='maximal')
    bus_peaks_max = bus_stops_peak_hours(feed, mode='maximal')
    bus_intersections_max = identify_bus_stop_intersections(feed, bus_peaks_max, mode='maximal')
    merged_max = merge_transit_stops(rail_ferry_brt_max, bus_intersections_max, mode='maximal', output_path=output_path_max)
    buffered_max = buffer_transit_stops(merged_max, mode='maximal', output_path=output_path_max)
    
    # Process minimal definition
    print("Processing minimal definition")
    rail_ferry_brt_min = rail_ferry_brt(feed, mode='minimal')
    bus_peaks_min = bus_stops_peak_hours(feed, mode='minimal')
    bus_intersections_min = identify_bus_stop_intersections(feed, bus_peaks_min, mode='minimal')
    merged_min = merge_transit_stops(rail_ferry_brt_min, bus_intersections_min, mode='minimal', output_path=output_path_min)
    buffered_min = buffer_transit_stops(merged_min, mode='minimal', output_path=output_path_min)
    
    print("Transit Zoning Pipeline completed successfully")
    
    return {
        'minimal': {
            'rail_ferry_brt': rail_ferry_brt_min,
            'bus_peaks': bus_peaks_min,
            'bus_intersections': bus_intersections_min,
            'merged': merged_min,
            'buffered': buffered_min
        },
        'maximal': {
            'rail_ferry_brt': rail_ferry_brt_max,
            'bus_peaks': bus_peaks_max,
            'bus_intersections': bus_intersections_max,
            'merged': merged_max,
            'buffered': buffered_max
        }
    }

if __name__ == "__main__":
    # Output path
    output_path = r"C:\Users\marce\OneDrive\Documents\UCLA ITS\Transit Zoning Project\Transit Job Output"
    
    # Since your GTFS paths are already defined at the top of the file (gtfs_path_2024), 
    # we can use that variable directly
    print(f"Starting process with {len(gtfs_path_2024)} GTFS files")
    print(f"Output will be saved to: {output_path}")
    
    # Run the pipeline
    results = run_transit_zoning_pipeline(gtfs_path_2024, output_path)
    
    print("Process completed successfully!")

    