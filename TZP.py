
# -*- coding: utf-8 -*-
"""Combined.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mLHNGl1ZIFT5VxOiS_DksYi_hs7sB5ht
""" 

import partridge as ptg
import pandas as pd
import numpy as np
import geopandas as gpd
from shapely.geometry import Point
from shapely.strtree import STRtree
from datetime import datetime
import os
import logging 
import zipfile
import io

logging.basicConfig(level=logging.INFO)

"""![flowchart](https://i.postimg.cc/Dy3gLM6f/image.png)

# Place file paths here

Replace these file paths with the ones you are using. Each file path should be a zipped or unzipped GTFS file.
"""
# Replace the file paths with your actual GTFS data
# gtfs_path_2024 = [
#     r"C:\Users\marce\Downloads\BART_Oct2024-20250211T180718Z-001.zip",
#     r"C:\Users\marce\Downloads\SFBayFerry_Oct2024-20250211T180832Z-001.zip",
#     r"C:\Users\marce\Downloads\VTA_Oct2024-20250211T180828Z-001.zip",
#     r"C:\Users\marce\Downloads\MTS_Oct2024-20250211T180803Z-001.zip",
#     r"C:\Users\marce\Downloads\NCTD_Oct2024-20250211T180813Z-001.zip",
#     r"C:\Users\marce\Downloads\OCTA_Oct2024-20250211T180806Z-001.zip",
#     r"C:\Users\marce\Downloads\Sac_Oct2024-20250211T180809Z-001.zip",
#     r"C:\Users\marce\Downloads\LAMetro_Oct2024-20250211T180746Z-001.zip",
#     r"C:\Users\marce\Downloads\MUNI_Oct2024-20250211T180756Z-001.zip",
#     r"C:\Users\marce\Downloads\LAMetroRail_Oct2024-20250211T180752Z-001.zip",
#     r"C:\Users\marce\Downloads\LADOT_Oct2024-20250211T180744Z-001.zip",
#     r"C:\Users\marce\Downloads\CC_Oct2024-20250211T180738Z-001.zip",
#     r"C:\Users\marce\Downloads\BBB_Oct2024-20250211T180731Z-001.zip",
#     r"C:\Users\marce\Downloads\ACT_Oct2024-20250211T180726Z-001.zip"
# ]

gtfs_path_2024 = [
    r"C:\Users\marce\Downloads\LAMetro_Oct2024-20250211T180746Z-001.zip",
    r"C:\Users\marce\Downloads\LAMetroRail_Oct2024-20250211T180752Z-001.zip",
    r"C:\Users\marce\Downloads\LADOT_Oct2024-20250211T180744Z-001.zip",
    r"C:\Users\marce\Downloads\BBB_Oct2024-20250211T180731Z-001.zip"
]
"""# Maximal/Minimal Toggle

If "maximal" is set to True, the maximal interpretation will be automatically applied throughout the notebook. If it is set to False, the minimal interpretation will be applied.
"""

maximal = True  # or 'maximal' when you want to switch modes

"""# Step #1: Loading GTFS

This step takes multiple GTFS files and combines them into one dataset.
"""

class GTFSFeed:
    def __init__(self, routes, trips, stop_times, stops, agency):  # Added agency parameter
        self.routes = routes
        self.trips = trips
        self.stop_times = stop_times
        self.stops = stops
        self.agency = agency

def load_and_combine_gtfs(gtfs_paths):
    # Initialize DataFrames
    combined_routes = pd.DataFrame()
    combined_trips = pd.DataFrame()
    combined_stop_times = pd.DataFrame()
    combined_stops = pd.DataFrame()
    combined_agency = pd.DataFrame()

    for path in gtfs_paths:
        prefix = os.path.splitext(os.path.basename(path))[0][:10]
        feed = ptg.load_feed(path)
        
        # Get copies of the dataframes
        routes = feed.routes.copy()
        trips = feed.trips.copy()
        stop_times = feed.stop_times.copy()
        stops = feed.stops.copy()
        
        # Add agency_name to routes
        routes['agency_name'] = prefix
        
        # ID handling
        routes['route_id'] = routes['route_id'].astype(str)
        trips['route_id'] = trips['route_id'].astype(str)
        trips['trip_id'] = trips['trip_id'].astype(str)
        stop_times['trip_id'] = stop_times['trip_id'].astype(str)
        stop_times['stop_id'] = stop_times['stop_id'].astype(str)
        stops['stop_id'] = stops['stop_id'].astype(str)

        # Prefix application
        routes['prefixed_route_id'] = prefix + "_" + routes['route_id']
        trips['prefixed_route_id'] = prefix + "_" + trips['route_id']
        trips['prefixed_trip_id'] = prefix + "_" + trips['trip_id']
        stop_times['prefixed_trip_id'] = prefix + "_" + stop_times['trip_id']
        stop_times['prefixed_stop_id'] = prefix + "_" + stop_times['stop_id']
        stops['prefixed_stop_id'] = prefix + "_" + stops['stop_id']
        
        # Create agency dataframe
        if hasattr(feed, 'agency'):
            agency_df = feed.agency.copy()
            agency_df['prefixed_agency_id'] = prefix + "_" + agency_df['agency_id'].astype(str)
        else:
            # Create a default agency record
            agency_df = pd.DataFrame({
                'agency_id': ['1'],
                'prefixed_agency_id': [prefix + "_1"],
                'agency_name': [prefix]
            })
        combined_agency = pd.concat([combined_agency, agency_df], ignore_index=True)
        
        # Add agency_name to trips from routes
        trips = trips.merge(
            routes[['prefixed_route_id', 'agency_name']],
            on='prefixed_route_id',
            how='left'
        )
        
        # Add agency_name to stop_times from trips
        stop_times = stop_times.merge(
            trips[['prefixed_trip_id', 'agency_name']],
            on='prefixed_trip_id',
            how='left'
        )
        
        # Add agency_name to stops from stop_times
        stop_agency_map = stop_times.groupby('prefixed_stop_id')['agency_name'].apply(
            lambda x: '; '.join(set(x.dropna()))
        ).to_dict()
        
        stops['agency_name'] = stops['prefixed_stop_id'].map(stop_agency_map)
        
        # Concatenate data
        combined_routes = pd.concat([combined_routes, routes], ignore_index=True)
        combined_trips = pd.concat([combined_trips, trips], ignore_index=True)
        combined_stop_times = pd.concat([combined_stop_times, stop_times], ignore_index=True)
        combined_stops = pd.concat([combined_stops, stops], ignore_index=True)

    # Print agency status for debugging
    print(f"Stops with agency_name: {combined_stops['agency_name'].notna().sum()}/{len(combined_stops)}")
    
    return GTFSFeed(
        routes=combined_routes,
        trips=combined_trips,
        stop_times=combined_stop_times,
        stops=combined_stops,
        agency=combined_agency
    )
# Assuming 'gtfs_paths' is your list of GTFS file paths
#step1 = load_and_combine_gtfs(gtfs_path_2024)


"""# Step #2: Rail, ferry, BRT stops

This step identifies transit stops that serve rail, light rail, subway, ferry, or some BRT routes. These routes-except for BRT-have their own designation
in GTFS files and all count for HQ transit stops. BRT routes are selected by name.
"""

def rail_ferry_brt(feed, mode = maximal):
    # Define route types for rail, light rail, and ferry services
    if not mode:
        rail_route_types = [0, 1, 5, 7]  # Light rail, subway, cable car, funicular
    else:
        rail_route_types = [0, 1, 2, 5, 7]  # Light rail, subway, intercity rail, cable car, funicular

    ferry_route_type = 4  # Ferry only

    # Filter routes for rail, light rail, and ferry
    rail_ferry_routes = feed.routes[
        feed.routes['route_type'].isin(rail_route_types + [ferry_route_type])
    ]

    # Include specific LA + SF BRT lines by route_long_name
    la_brt_routes = feed.routes[
        feed.routes['route_long_name'].isin(["Metro G Line 901", "Metro J Line 910/950", "GEARY RAPID"])
    ]

    # Combine rail, ferry, and BRT routes
    all_relevant_routes = pd.concat([rail_ferry_routes, la_brt_routes])

    # Ensure 'route_id' matches between routes and trips
    relevant_trips = feed.trips[
        feed.trips['prefixed_route_id'].isin(all_relevant_routes['prefixed_route_id'])
    ]

    # Ensure 'trip_id' matches between trips and stop_times
    relevant_stop_times = feed.stop_times[
        feed.stop_times['prefixed_trip_id'].isin(relevant_trips['prefixed_trip_id'])
    ]

    # Ensure 'stop_id' matches between stop_times and stops
    relevant_stops = feed.stops[
        feed.stops['prefixed_stop_id'].isin(relevant_stop_times['prefixed_stop_id'])
    ]

    # Create GeoDataFrame for spatial analysis
    rail_ferry_brt_gdf = gpd.GeoDataFrame(
        relevant_stops,
        geometry=gpd.points_from_xy(relevant_stops.stop_lon, relevant_stops.stop_lat),
        crs="EPSG:4326"  # Best projection for the area
    )

    print("Rail, ferry, and BRT stops extracted.")
    return rail_ferry_brt_gdf
#step2 = rail_ferry_brt(step1)



"""# Step #3: Bus stops with peak hours

This step identifies bus stops with frequent service during peak hours (morning and afternoon). In the maximal definition, a stop qualifies
if it has, from one route, at least one hour-long period with 3 arrivals throughout the morning and afternoon peak hours. This hour-long period need not start at the beginning of an hour; for example, it could stretch from 7:30-8:30AM. In the minimal definition, a stop qualifies if it has, from one route, at least 9 buses arriving during the morning peak AND 12 buses in the afternoon peak. The morning peak is 6-9AM and the afternoon peak is 3-7PM.
"""

def bus_stops_peak_hours(feed, mode = maximal):
    print("Starting bus stops peak hours analysis...")
    
    if not mode:  # minimal mode
        print("Using minimal mode")
        # Get bus routes
        # @MARCEL - these lines are repeated in both maximal and minimal mode. Move above the if statement?
        bus_routes = feed.routes[feed.routes['route_type'] == 3]
        bus_trips = feed.trips[feed.trips['prefixed_route_id'].isin(bus_routes['prefixed_route_id'])]
        
        # Get stop times and convert to datetime
        stop_times = feed.stop_times.merge(
            bus_trips[['prefixed_trip_id', 'prefixed_route_id']], 
            on='prefixed_trip_id'
        )
        
        # Convert seconds to hours for easier filtering
        stop_times['hour'] = pd.to_datetime(stop_times['arrival_time'], unit='s').dt.hour
        
        # Filter for peak periods
        am_peak = stop_times[(stop_times['hour'] >= 6) & (stop_times['hour'] < 9)]
        pm_peak = stop_times[(stop_times['hour'] >= 15) & (stop_times['hour'] < 19)]
        
        # Count by route and stop
        am_counts = am_peak.groupby(['prefixed_stop_id', 'prefixed_route_id']).size()
        pm_counts = pm_peak.groupby(['prefixed_stop_id', 'prefixed_route_id']).size()
        
        # Get qualifying stops (9+ AM and 12+ PM on same route)
        qualifying_routes_am = am_counts[am_counts >= 9].reset_index()
        qualifying_routes_pm = pm_counts[pm_counts >= 12].reset_index()
        
        # Find stops that meet both criteria on the same route
        qualifying_stops = pd.merge(
            qualifying_routes_am[['prefixed_stop_id', 'prefixed_route_id']],
            qualifying_routes_pm[['prefixed_stop_id', 'prefixed_route_id']],
            on=['prefixed_stop_id', 'prefixed_route_id']
        )['prefixed_stop_id'].unique()
        
    else:  # maximal mode
        print("Using maximal mode")
        # Get bus routes
        bus_routes = feed.routes[feed.routes['route_type'] == 3]
        bus_trips = feed.trips[feed.trips['prefixed_route_id'].isin(bus_routes['prefixed_route_id'])]
        
        # Get stop times and convert to datetime
        stop_times = feed.stop_times.merge(
            bus_trips[['prefixed_trip_id', 'prefixed_route_id']], 
            on='prefixed_trip_id'
        )
        # @marcel don't we need to do this separately for AM and PM, and make sure that a qualifying stop has at least one hour in BOTH periods?

        # Convert to datetime for rolling hour windows
        stop_times['time'] = pd.to_datetime(stop_times['arrival_time'], unit='s')
        
        # Group by stop and route, then check each hour window
        qualifying_stops = set()
        for (stop_id, route_id), group in stop_times.groupby(['prefixed_stop_id', 'prefixed_route_id']):
            # Sort by time and resample to rolling hour windows
            times = group['time'].sort_values()
            for start_time in times:
                end_time = start_time + pd.Timedelta(hours=1)
                count = sum((times >= start_time) & (times < end_time))
                print(count)
                if count >= 3:
                    qualifying_stops.add(stop_id)
                    break

    print(f"Found {len(qualifying_stops)} qualifying stops")
    
    # Get final stops and convert to GeoDataFrame
    bus_peak_stops = feed.stops[feed.stops['prefixed_stop_id'].isin(qualifying_stops)]
    
    return gpd.GeoDataFrame(
        bus_peak_stops,
        geometry=gpd.points_from_xy(bus_peak_stops.stop_lon, bus_peak_stops.stop_lat),
        crs="EPSG:4326"
    )
"""# Step #4: Bus stop intersections

This step finds bus stops that are near each other (within 150 or 500 feet) but serve different bus routes. This prevents stops on the same route
from intersecting, which Cal-ITP believed was against the intent of the law.
"""



"""
    Identifies intersecting bus stops based on a distance threshold, excluding stops on the same route.
    
    Parameters:
    - feed: GTFS feed containing routes, trips, and stop_times
    - bus_peak_gdf: GeoDataFrame with bus stops meeting peak hour criteria
    - mode: 'minimal' or 'maximal' mode
    
    Returns:
    - GeoDataFrame with qualifying intersecting bus stops
    """
def identify_bus_stop_intersections(feed, bus_peak_gdf, mode = maximal):
    print(f"Starting intersection detection with {len(bus_peak_gdf)} stops...")
    
    # Project to UTM Zone 11N for accurate distance measurement
    bus_peak_gdf = bus_peak_gdf.to_crs(epsg=32611)
    print("Projection complete")

    # Set distance threshold in meters (150ft = 45.72m, 500ft = 152.4m)
    distance_threshold = 45.72 if not mode else 152.4

    # Join route information to stops
    print("Joining route information...")
    # @marcel need to use prefixed version of these columns...stop_ids are only unique within agency, I believe
    stop_route_data = (
        bus_peak_gdf.merge(feed.stop_times[['stop_id', 'trip_id']], on='stop_id')
        .merge(feed.trips[['trip_id', 'route_id']], on='trip_id')
        .merge(feed.routes[['route_id', 'route_short_name']], on='route_id')
        .drop_duplicates(['stop_id', 'route_id'])
    )
    print(f"Route information joined. Processing {len(stop_route_data)} stop-route combinations")

    # Create spatial index
    spatial_index = stop_route_data.sindex  # @marcel - may not be necessary. See suggestion below
    intersecting_stops = set()
    
    # Process in chunks to show progress
    total = len(stop_route_data)
    for i, (idx, stop_a) in enumerate(stop_route_data.iterrows()):
        if i % 1000 == 0:
            print(f"Processing stop {i}/{total} ({(i/total)*100:.1f}%)")

        # @marcel - alternative to the code below. Confirm whether it is faster and gets identical results?
        # get stops within range
        possible_matches = stop_route_data[stop_route_data.geometry.dwithin(stop_a.geometry, distance_threshold)]
        # exclude stops where the stop ID OR the route name is the same
        # @marcel - should we use prefixed route name?
        possible_matches = possible_matches[(possible_matches.prefixed_stop_id!=stop_a.prefixed_stop_id) & (possible_matches.route_short_name!=stop_a.route_short_name)]
        if len(possible_matches)>0:
            intersecting_stops.update([stop_a['prefixed_stop_id']])
            intersecting_stops.update(possible_matches['prefixed_stop_id'].unique())


        # Find potential matches
        possible_matches = list(spatial_index.intersection(stop_a.geometry.buffer(distance_threshold).bounds))


        for match_idx in possible_matches:
            stop_b = stop_route_data.iloc[match_idx]
            
            # Skip same stop or same route
            # @marcel - should we have a prefix for the route short name?
            if stop_a.name == stop_b.name or stop_a['route_short_name'] == stop_b['route_short_name']:
                continue

            # Calculate distance
            distance = stop_a.geometry.distance(stop_b.geometry)
            if distance <= distance_threshold:
                intersecting_stops.update([stop_a['stop_id'], stop_b['stop_id']])

    print(f"Found {len(intersecting_stops)} intersecting stops")
    # @marcel - maybe prefixed_stop_id? same in the function merge_transit_stops
    return bus_peak_gdf[bus_peak_gdf['stop_id'].isin(intersecting_stops)]


"""# Step #5: Merge datasets

This step combines the rail/ferry stops and high-quality bus stops (from steps 2 and 4) into one dataset. It then exports this dataset as a shapefile of the stops (without 1/2 mile buffers) to a designated file path.
"""
def merge_transit_stops(rail_ferry_brt, bus_intersections_gdf, mode=maximal,
                        output_path=r"C:\Users\marce\OneDrive\Documents\UCLA ITS\Transit Zoning Project\Transit Job Output"):
    # Ensure all GeoDataFrames are in the same CRS
    if rail_ferry_brt.crs != bus_intersections_gdf.crs:
        bus_intersections_gdf = bus_intersections_gdf.to_crs(rail_ferry_brt.crs)

    # Add qualification labels
    rail_ferry_brt['qualification'] = 'Rail, Ferry, BRT stop'
    distance = 150 if not mode else 500
    bus_intersections_gdf['qualification'] = f'Bus stop with intersection within {distance} feet'

    # Combine datasets
    all_high_quality_stops_gdf = gpd.GeoDataFrame(
        pd.concat([rail_ferry_brt, bus_intersections_gdf], ignore_index=True)
    )

    # Only aggregate columns that exist
    agg_dict = {
        'qualification': lambda x: '; '.join(set(x)),
        'geometry': 'first',
        'stop_lat': 'first',
        'stop_lon': 'first',
        'stop_name': 'first'
    }

    # Add agency_name to aggregation only if it exists
    if 'agency_name' in all_high_quality_stops_gdf.columns:
        agg_dict['agency_name'] = lambda x: '; '.join(set(x.dropna()))

    # Aggregate stops
    all_stops_aggregated = all_high_quality_stops_gdf.groupby('stop_id', as_index=False).agg(agg_dict)

    # Convert to GeoDataFrame
    result = gpd.GeoDataFrame(all_stops_aggregated, geometry='geometry', crs="EPSG:4326")
    
    # Rename columns to ensure they work in shapefiles (max 10 chars)
    # @marcel - use a GPKG to avoid this constraint
    if 'agency_name' in result.columns:
        result.rename(columns={'agency_name': 'agency_nm'}, inplace=True)
        # Ensure agency_nm is properly formatted for shapefiles
        result['agency_nm'] = result['agency_nm'].astype(str).str[:254]
    
    # Create output directory if it doesn't exist
    os.makedirs(output_path, exist_ok=True)
    # @marcel - put maximal or minimal in the filename
    result.to_file(os.path.join(output_path, "file2.shp"), driver='ESRI Shapefile')

    return result

# Run the function with the updated code
#step5 = merge_transit_stops(step2, step4, mode='maximal')
"""# Step #6: Buffer transit stops

This step creates a buffer zone (1/2 mile) around each of the transit stops. These are the high-quality transit areas resulting from
the stops. This step exports the buffer zone to a designated file path.
"""
def buffer_transit_stops(high_quality_stops_gdf, buffer_distance_miles = 0.5,
                         output_path=r"C:\Users\marce\OneDrive\Documents\UCLA ITS\Transit Zoning Project\Transit Job Output"):
    """
    Buffer 1/2 mile around each stop and save to shapefile.
    """
    # Convert buffer distance from miles to meters
    buffer_distance_meters = buffer_distance_miles * 1609.34

    # Use UTM Zone 11N for buffering accuracy (best for buffering, I learned)
    if high_quality_stops_gdf.crs.to_epsg() != 32611:  # UTM Zone 11N
        high_quality_stops_gdf = high_quality_stops_gdf.to_crs(epsg=32611)

    # Apply the buffer and set it as the active geometry
    # @marcel - or skip the steps below and just do 
    #      high_quality_stops_gdf.geometry = high_quality_stops_gdf.geometry.buffer(buffer_distance_meters)
    high_quality_stops_gdf['buffer'] = high_quality_stops_gdf.geometry.buffer(buffer_distance_meters)
    buffered_stops_gdf = high_quality_stops_gdf.set_geometry('buffer').copy()

    # Drop the original geometry to avoid conflicts when saving
    buffered_stops_gdf = buffered_stops_gdf.drop(columns='geometry')

    # Save to file
    # Don't forget to comment out if not saving!
    # marcel - save with maximal or minimal in filename?
    buffered_stops_gdf.to_file(f"{output_path}/file_buffer2.shp", driver='ESRI Shapefile')

    return buffered_stops_gdf

if __name__ == "__main__":
    print("Starting Transit Zoning Project processing...")
    
    # Define your gtfs_paths
    gtfs_path_2024 = [
        r"C:\Users\marce\Downloads\LAMetro_Oct2024-20250211T180746Z-001.zip",
        r"C:\Users\marce\Downloads\LAMetroRail_Oct2024-20250211T180752Z-001.zip",
        r"C:\Users\marce\Downloads\LADOT_Oct2024-20250211T180744Z-001.zip",
        r"C:\Users\marce\Downloads\BBB_Oct2024-20250211T180731Z-001.zip"
    ]
    
    print("Running step 1: Loading GTFS")
    step1 = load_and_combine_gtfs(gtfs_path_2024)
    print("Running step 2: Identifying rail/ferry/BRT stops")
    step2 = rail_ferry_brt(step1)
    print("Running step 3: Identifying high-frequency bus stops")
    step3 = bus_stops_peak_hours(step1)
    print("Running step 4: Finding bus stop intersections")
    step4 = identify_bus_stop_intersections(step1, step3)
    print("Running step 5: Merging transit stops")
    step5 = merge_transit_stops(step2, step4)
    print("Running step 6: Creating buffers")
    step6 = buffer_transit_stops(step5)