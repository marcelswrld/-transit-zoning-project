
# -*- coding: utf-8 -*-
"""Combined.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mLHNGl1ZIFT5VxOiS_DksYi_hs7sB5ht
""" 

import partridge as ptg
import pandas as pd
import numpy as np
import geopandas as gpd
from shapely.geometry import Point
from shapely.strtree import STRtree
from datetime import datetime
import os
import logging 
import zipfile
import io

logging.basicConfig(level=logging.INFO)

"""![flowchart](https://i.postimg.cc/Dy3gLM6f/image.png)

# Place file paths here

Replace these file paths with the ones you are using. Each file path should be a zipped or unzipped GTFS file.
"""
# Replace the file paths with your actual GTFS data
gtfs_path_2024 = [
    r"C:\Users\marce\Downloads\BART_Oct2024-20250211T180718Z-001.zip",
    r"C:\Users\marce\Downloads\SFBayFerry_Oct2024-20250211T180832Z-001.zip",
    r"C:\Users\marce\Downloads\VTA_Oct2024-20250211T180828Z-001.zip",
    r"C:\Users\marce\Downloads\MTS_Oct2024-20250211T180803Z-001.zip",
    r"C:\Users\marce\Downloads\NCTD_Oct2024-20250211T180813Z-001.zip",
    r"C:\Users\marce\Downloads\OCTA_Oct2024-20250211T180806Z-001.zip",
    r"C:\Users\marce\Downloads\Sac_Oct2024-20250211T180809Z-001.zip",
    r"C:\Users\marce\Downloads\LAMetro_Oct2024-20250211T180746Z-001.zip",
    r"C:\Users\marce\Downloads\MUNI_Oct2024-20250211T180756Z-001.zip",
    r"C:\Users\marce\Downloads\LAMetroRail_Oct2024-20250211T180752Z-001.zip",
    r"C:\Users\marce\Downloads\LADOT_Oct2024-20250211T180744Z-001.zip",
    r"C:\Users\marce\Downloads\CC_Oct2024-20250211T180738Z-001.zip",
    r"C:\Users\marce\Downloads\BBB_Oct2024-20250211T180731Z-001.zip",
    r"C:\Users\marce\Downloads\ACT_Oct2024-20250211T180726Z-001.zip"
]
"""# Maximal/Minimal Toggle

If "maximal" is set to True, the maximal interpretation will be automatically applied throughout the notebook. If it is set to False, the minimal interpretation will be applied.
"""

mode = 'maximal'  # or 'maximal' when you want to switch modes

"""# Step #1: Loading GTFS

This step takes multiple GTFS files and combines them into one dataset.
"""

class GTFSFeed:
    """
    A custom GTFS object for handling concatenated GTFS tables.
    """
    def __init__(self, routes, trips, stop_times, stops, agency, frequencies):
        self.routes = routes
        self.trips = trips
        self.stop_times = stop_times
        self.stops = stops       
        self.agency = agency
        self.frequencies = frequencies

# Helper function to handle column name variations
def get_key(df, options):
    """Find which key in options exists in df.columns"""
    for key in options:
        if key in df.columns:
            return key
    return None  # No matching column found

def load_and_combine_gtfs(gtfs_paths):
    """
    Loads multiple GTFS files and combines them into a single object.
    """
    # Initialize empty DataFrames for each GTFS table
    combined_routes = pd.DataFrame()
    combined_trips = pd.DataFrame()
    combined_stop_times = pd.DataFrame()
    combined_stops = pd.DataFrame()
    combined_agency = pd.DataFrame()
    combined_frequencies = pd.DataFrame()

    # Loop through each GTFS file and append data
    for path in gtfs_paths:
        prefix = os.path.splitext(os.path.basename(path))[0][:10]
        print(f"\nProcessing feed: {path}")
        feed = ptg.load_feed(path, view=None)
        print("Feed loaded successfully.")

        # Extract DataFrames from the Feed object
        routes = feed.routes.copy()
        trips = feed.trips.copy()
        stop_times = feed.stop_times.copy()
        stops = feed.stops.copy()

        # Ensure ID columns are strings before applying prefixes
        id_columns = ['agency_id', 'route_id', 'trip_id', 'stop_id']
        for df in [routes, trips, stop_times, stops]:
            for col in id_columns:
                if col in df.columns:
                    df[col] = df[col].astype(str)

        # Initialize agency and frequencies DataFrames
        agency_df = pd.DataFrame()
        frequencies_df = pd.DataFrame()

        # Open the ZIP file and find the agency and frequencies files
        with zipfile.ZipFile(path, 'r') as zip_ref:
            zip_files = zip_ref.namelist()

            # Exclude hidden MacOS files (._ files)
            valid_files = [
                f for f in zip_files
                if not os.path.basename(f).startswith('._')
            ]

            # Ensure case-insensitive matching for filenames
            # Process agency.txt
            agency_files = [
                f for f in valid_files
                if os.path.basename(f).lower() == 'agency.txt'
            ]

            # Process frequencies.txt
            frequency_files = [
                f for f in valid_files
                if os.path.basename(f).lower() == 'frequencies.txt'
            ]

            # Process agency.txt
            if agency_files:
                agency_filename = agency_files[0]  # Use the first match
                print(f"Reading agency file: {agency_filename}")
                # Read agency file from ZIP
                with zip_ref.open(agency_filename) as agency_file:
                    try:
                        agency_df = pd.read_csv(agency_file)
                        print("Agency data loaded successfully.")
                    except Exception as e:
                        print(f"Error reading '{agency_filename}' in '{path}': {e}")
                        agency_df = pd.DataFrame()
                if not agency_df.empty and 'agency_id' in agency_df.columns:
                    agency_df['agency_id'] = agency_df['agency_id'].astype(str).fillna('1')
                    agency_df['prefixed_agency_id'] = prefix + "_" + agency_df['agency_id']
                else:
                    print(f"'agency_id' missing in agency_df for {path}. Assigning default '1'.")
                    agency_df = pd.DataFrame({
                        'agency_id': ['1'],
                        'prefixed_agency_id': [prefix + "_1"],
                        'agency_name': ['Unknown Agency']
                    })
                # Combine agency data
                combined_agency = pd.concat([combined_agency, agency_df], ignore_index=True)
            else:
                print(f"Warning: 'agency.txt' not found in {path}. Proceeding without agency data.")
                # Assign default agency data
                agency_df = pd.DataFrame({
                    'agency_id': ['1'],
                    'prefixed_agency_id': [prefix + "_1"],
                    'agency_name': ['Unknown Agency']
                })
                combined_agency = pd.concat([combined_agency, agency_df], ignore_index=True)

            # Process frequencies.txt
            if frequency_files:
                frequency_filename = frequency_files[0]  # Use the first match
                print(f"Reading frequencies file: {frequency_filename}")
                # Read frequencies file from ZIP
                with zip_ref.open(frequency_filename) as frequency_file:
                    try:
                        frequencies_df = pd.read_csv(frequency_file)
                        print("Frequencies data loaded successfully.")
                    except Exception as e:
                        print(f"Error reading '{frequency_filename}' in '{path}': {e}")
                        frequencies_df = pd.DataFrame()
                if not frequencies_df.empty and 'trip_id' in frequencies_df.columns:
                    frequencies_df['trip_id'] = frequencies_df['trip_id'].astype(str)
                    frequencies_df['prefixed_trip_id'] = prefix + "_" + frequencies_df['trip_id']
                else:
                    frequencies_df = pd.DataFrame()
                # Combine frequencies data
                combined_frequencies = pd.concat([combined_frequencies, frequencies_df], ignore_index=True)
            else:
                print(f"Warning: 'frequencies.txt' not found in {path}. Proceeding without frequencies data.")
                frequencies_df = pd.DataFrame()

        # Ensure 'agency_id' exists in routes
        if 'agency_id' in routes.columns and not routes['agency_id'].isnull().all():
            routes['agency_id'] = routes['agency_id'].astype(str).fillna('1')
        else:
            # Assign agency_id from agency_df or default '1'
            default_agency_id = agency_df['agency_id'].iloc[0] if not agency_df.empty else '1'
            routes['agency_id'] = default_agency_id
            print(f"`agency_id` assigned as '{default_agency_id}' for routes in {path}.")

        # Create 'prefixed_agency_id' in both agency_df and routes
        agency_df['prefixed_agency_id'] = prefix + "_" + agency_df['agency_id']
        routes['prefixed_agency_id'] = prefix + "_" + routes['agency_id']

        # Apply prefixes to other IDs
        routes['prefixed_route_id'] = prefix + "_" + routes['route_id']
        trips['prefixed_route_id'] = prefix + "_" + trips['route_id']
        trips['prefixed_trip_id'] = prefix + "_" + trips['trip_id']
        stop_times['prefixed_trip_id'] = prefix + "_" + stop_times['trip_id']
        stop_times['prefixed_stop_id'] = prefix + "_" + stop_times['stop_id']
        stops['prefixed_stop_id'] = prefix + "_" + stops['stop_id']

        # Merge 'agency_name' into routes
        routes = routes.merge(
            agency_df[['prefixed_agency_id', 'agency_name']],
            on='prefixed_agency_id',
            how='left',
            indicator=False
        )

        # **Ensure 'prefixed_agency_id' is preserved in routes**
        if 'prefixed_agency_id' not in routes.columns:
            print(f"'prefixed_agency_id' not in routes after merging for {path}.")
        else:
            print(f"'prefixed_agency_id' is present in routes after merging for {path}.")

        # **Merge 'agency_name' and 'prefixed_agency_id' into trips**
        trips = trips.merge(
            routes[['prefixed_route_id', 'prefixed_agency_id', 'agency_name']],
            on='prefixed_route_id',
            how='left'
        )

        # Append to combined DataFrames
        combined_routes = pd.concat([combined_routes, routes], ignore_index=True)
        combined_trips = pd.concat([combined_trips, trips], ignore_index=True)
        combined_stop_times = pd.concat([combined_stop_times, stop_times], ignore_index=True)
        combined_stops = pd.concat([combined_stops, stops], ignore_index=True)

    # After processing all feeds, merge frequencies into combined_trips
    if not combined_frequencies.empty:
        # Ensure 'prefixed_trip_id' is a string
        combined_frequencies['prefixed_trip_id'] = combined_frequencies['prefixed_trip_id'].astype(str)
        combined_trips['prefixed_trip_id'] = combined_trips['prefixed_trip_id'].astype(str)

        # Merge frequencies into trips
        combined_trips = combined_trips.merge(
            combined_frequencies[['prefixed_trip_id', 'headway_secs']],
            on='prefixed_trip_id',
            how='left'
        )
    else:
        combined_trips['headway_secs'] = None  # Add column with None if frequencies are missing

    # Return a GTFSFeed object
    return GTFSFeed(
        routes=combined_routes,
        trips=combined_trips,
        stop_times=combined_stop_times,
        stops=combined_stops,
        agency=combined_agency,
        frequencies=combined_frequencies
    )

# Assuming 'gtfs_paths' is your list of GTFS file paths
#step1 = load_and_combine_gtfs(gtfs_path_2024)


"""# Step #2: Rail, ferry, BRT stops

This step identifies transit stops that serve rail, light rail, subway, ferry, or some BRT routes. These routes-except for BRT-have their own designation
in GTFS files and all count for HQ transit stops. BRT routes are selected by name.
"""

def rail_ferry_brt(feed, mode='maximal'):
    """
    Identifies rail, ferry, and selected BRT stops from GTFS data.

    Params:
        feed: GTFSFeed object containing combined GTFS tables.
        mode: 'minimal' or 'maximal' to define route types.

    Returns:
        GeoDataFrame with rail, ferry, and BRT stops including agency names and frequencies.
    """

    # Define route types based on mode
    if mode == 'minimal':
        rail_route_types = [0, 1, 5, 7]  # Tram, Subway, Cable Car, Funicular
    else:
        rail_route_types = [0, 1, 2, 5, 7]  # Including Intercity Rail

    ferry_route_type = 4  # Ferry

    # ✅ Filter routes for rail and ferry
    rail_ferry_routes = feed.routes[
        feed.routes['route_type'].isin(rail_route_types + [ferry_route_type])
    ]

    # ✅ Include specific BRT lines by route_long_name
    brt_lines = ["Metro G Line 901", "Metro J Line 910/950", "GEARY RAPID"]
    brt_routes = feed.routes[
        feed.routes['route_long_name'].isin(brt_lines)
    ]

    # ✅ Combine rail, ferry, and BRT routes
    all_routes = pd.concat([rail_ferry_routes, brt_routes], ignore_index=True)

    # ✅ Ensure only existing columns are selected for merging
    columns_to_merge = ['prefixed_route_id']
    if 'prefixed_agency_id' in feed.routes.columns:
        columns_to_merge.append('prefixed_agency_id')

    # ✅ Merge 'prefixed_agency_id' into all_routes safely
    all_routes = all_routes.merge(
        feed.routes[columns_to_merge],
        on='prefixed_route_id',
        how='left'
    )

    # ✅ Debugging: Check for missing columns before merging
    print("✅ Columns in all_routes after merging:", all_routes.columns.tolist())

    # Get trips for the selected routes
    relevant_trips = feed.trips[
        feed.trips['prefixed_route_id'].isin(all_routes['prefixed_route_id'])
    ].copy()
    
    # Use get_key to find the right column names
    trip_key = get_key(relevant_trips, ['prefixed_trip_id', 'trip_id'])
    agency_key = get_key(relevant_trips, ['prefixed_agency_id', 'agency_id'])
    
    # Create a list of columns that exist in relevant_trips
    available_columns = []
    if trip_key:
        available_columns.append(trip_key)
    if agency_key:
        available_columns.append(agency_key)
    if 'agency_name' in relevant_trips.columns:
        available_columns.append('agency_name')
    if 'headway_secs' in relevant_trips.columns:
        available_columns.append('headway_secs')
    
    # Only use columns that actually exist
    relevant_stop_times = feed.stop_times[
        feed.stop_times['prefixed_trip_id'].isin(relevant_trips['prefixed_trip_id'])
    ].copy()
    
    # Merge only using columns that exist
    if available_columns:
        relevant_stop_times = relevant_stop_times.merge(
            relevant_trips[available_columns],
            on='prefixed_trip_id',
            how='left'
        )

    # ✅ Get stop_times for the relevant trips
    relevant_stop_times = feed.stop_times[
        feed.stop_times['prefixed_trip_id'].isin(relevant_trips['prefixed_trip_id'])
    ].copy()

    # ✅ Merge agency_name, headway_secs, and prefixed_agency_id into stop_times
    relevant_stop_times = relevant_stop_times.merge(
        relevant_trips[['prefixed_trip_id', 'prefixed_agency_id', 'agency_name', 'headway_secs']],
        on='prefixed_trip_id',
        how='left'
    )

    # ✅ Get stops for the relevant stop_times
    relevant_stops = feed.stops[
        feed.stops['prefixed_stop_id'].isin(relevant_stop_times['prefixed_stop_id'])
    ].copy()

    # ✅ Merge agency_name, headway_secs, and prefixed_agency_id into stops
    relevant_stops = relevant_stops.merge(
        relevant_stop_times[['prefixed_stop_id', 'prefixed_agency_id', 'agency_name', 'headway_secs']].drop_duplicates(),
        on='prefixed_stop_id',
        how='left'
    )

    # ✅ Create GeoDataFrame
    rail_ferry_brt_gdf = gpd.GeoDataFrame(
        relevant_stops,
        geometry=gpd.points_from_xy(relevant_stops['stop_lon'], relevant_stops['stop_lat']),
        crs="EPSG:4326"
    )

    print("✅ Rail, ferry, and BRT stops extracted with agency names and frequencies.")
    return rail_ferry_brt_gdf

# ✅ Run the function
#step2 = rail_ferry_brt(step1)



"""# Step #3: Bus stops with peak hours

This step identifies bus stops with frequent service during peak hours (morning and afternoon). In the maximal definition, a stop qualifies
if it has, from one route, at least one hour-long period with 3 arrivals throughout the morning and afternoon peak hours. This hour-long period need not start at the beginning of an hour; for example, it could stretch from 7:30-8:30AM. In the minimal definition, a stop qualifies if it has, from one route, at least 9 buses arriving during the morning peak AND 12 buses in the afternoon peak. The morning peak is 6-9AM and the afternoon peak is 3-7PM.
"""

def bus_stops_peak_hours(feed, mode='maximal'):
    """
    Identifies bus stops with frequent service during peak hours.
    """
    import logging
    logging.info(f"Processing bus stops for {mode} mode")

    # Define peak periods
    if mode == 'minimal':
        # For minimal mode, use strict peak hours
        am_start_hour = 6
        am_end_hour = 9
        pm_start_hour = 15
        pm_end_hour = 19
        threshold_am = 9
        threshold_pm = 12
    else:
        # For maximal mode, use full day
        am_start_hour = 0
        am_end_hour = 12
        pm_start_hour = 12
        pm_end_hour = 24
        threshold_am = 9
        threshold_pm = 12

    # Key handling
    route_key = get_key(feed.routes, ['prefixed_route_id', 'route_id'])
    trip_key = get_key(feed.trips, ['prefixed_trip_id', 'trip_id'])
    stop_key = get_key(feed.stops, ['prefixed_stop_id', 'stop_id'])
    agency_key = get_key(feed.routes, ['prefixed_agency_id', 'agency_id'])

    # Only select bus routes
    bus_routes = feed.routes[feed.routes['route_type'] == 3].copy()

    # Get agency information
    if agency_key and 'agency_name' not in bus_routes.columns:
        # Try to get agency information from feed.agency
        if hasattr(feed, 'agency') and not feed.agency.empty and 'agency_name' in feed.agency.columns:
            bus_routes = bus_routes.merge(
                feed.agency[[agency_key, 'agency_name']], 
                on=agency_key, 
                how='left'
            )
        else:
            # Add default agency name if not available
            bus_routes['agency_name'] = 'Unknown Agency'

    # Get trips for bus routes
    bus_trips = feed.trips[feed.trips[route_key].isin(bus_routes[route_key])].copy()

    # Enrich stop_times with route information
    stop_times_enriched = feed.stop_times.merge(
        bus_trips[[trip_key, route_key]],
        on=trip_key,
        how='inner'
    )

    # Convert arrival_time correctly - handle it as seconds from midnight
    def convert_gtfs_time(time_str):
        try:
            h, m, s = map(int, time_str.split(':'))
            return h * 3600 + m * 60 + s  # Convert to seconds
        except (ValueError, AttributeError):
            # If already a number (seconds), return it
            if isinstance(time_str, (int, float)):
                return time_str
            return 0

    # Apply the conversion
    stop_times_enriched['arrival_time_seconds'] = stop_times_enriched['arrival_time'].apply(convert_gtfs_time)

    # Function to compute trips per hour using arrival_time_seconds
    def compute_trips_per_hour(df, start_hour, end_hour):
        # Convert hours to seconds
        start_seconds = start_hour * 3600
        end_seconds = end_hour * 3600 - 1  # End at 23:59:59 for example
        
        # Filter by time range
        peak_df = df[(df['arrival_time_seconds'] >= start_seconds) & 
                     (df['arrival_time_seconds'] <= end_seconds)].copy()
        
        if peak_df.empty:
            logging.warning(f"No trips found between {start_hour}:00 and {end_hour}:00")
            return pd.DataFrame()
        
        # Group by hour (using integer division)
        peak_df['hour'] = peak_df['arrival_time_seconds'] // 3600
        
        # Count trips per stop, route, and hour
        trips_per_hour = peak_df.groupby([stop_key, route_key, 'hour']).size().reset_index(name='trips')
        return trips_per_hour

    # Compute trips for AM and PM periods
    trips_per_hour_am = compute_trips_per_hour(stop_times_enriched, am_start_hour, am_end_hour)
    trips_per_hour_pm = compute_trips_per_hour(stop_times_enriched, pm_start_hour, pm_end_hour)

    if trips_per_hour_am.empty or trips_per_hour_pm.empty:
        logging.error("No trips found in one or both peak periods. Check GTFS data.")
        return gpd.GeoDataFrame()

    # Aggregate trips by stop and route
    total_trips_am = trips_per_hour_am.groupby([stop_key, route_key])['trips'].sum().reset_index()
    total_trips_am.rename(columns={'trips': 'total_trips_am'}, inplace=True)
    
    total_trips_pm = trips_per_hour_pm.groupby([stop_key, route_key])['trips'].sum().reset_index()
    total_trips_pm.rename(columns={'trips': 'total_trips_pm'}, inplace=True)

    # Combine AM and PM data
    total_trips = total_trips_am.merge(
        total_trips_pm, 
        on=[stop_key, route_key], 
        how='outer'
    ).fillna(0)

    # Add agency information if available
    if 'agency_name' in bus_routes.columns:
        total_trips = total_trips.merge(
            bus_routes[[route_key, 'agency_name']],
            on=route_key,
            how='left'
        )

    # Apply qualification criteria based on mode
    if mode == 'minimal':
        # For minimal, each individual route must meet thresholds
        qualifying_trips = total_trips[
            (total_trips['total_trips_am'] >= threshold_am) &
            (total_trips['total_trips_pm'] >= threshold_pm)
        ]
    else:
        # For maximal, sum across routes at each stop
        stop_total_trips = total_trips.groupby(stop_key).agg({
            'total_trips_am': 'sum',
            'total_trips_pm': 'sum'
        }).reset_index()
        
        qualifying_stops = stop_total_trips[
            (stop_total_trips['total_trips_am'] >= threshold_am) &
            (stop_total_trips['total_trips_pm'] >= threshold_pm)
        ][stop_key]
        
        qualifying_trips = total_trips[total_trips[stop_key].isin(qualifying_stops)]

    # Get unique qualified stop IDs
    qualifying_stop_ids = qualifying_trips[stop_key].unique()
    
    if len(qualifying_stop_ids) == 0:
        logging.warning("No stops meet the frequency thresholds.")
        return gpd.GeoDataFrame()

    # Get stop information for qualifying stops
    qualifying_stops = feed.stops[feed.stops[stop_key].isin(qualifying_stop_ids)].copy()
    
    # Create GeoDataFrame
    qualifying_stops_gdf = gpd.GeoDataFrame(
        qualifying_stops,
        geometry=gpd.points_from_xy(qualifying_stops['stop_lon'], qualifying_stops['stop_lat']),
        crs="EPSG:4326"
    )
    
    # Add frequency and agency information
    for idx, stop in qualifying_stops_gdf.iterrows():
        stop_trips = qualifying_trips[qualifying_trips[stop_key] == stop[stop_key]]
        if not stop_trips.empty:
            if 'agency_name' in stop_trips.columns:
                agencies = '; '.join(stop_trips['agency_name'].dropna().unique())
                qualifying_stops_gdf.loc[idx, 'agency_name'] = agencies
            
            qualifying_stops_gdf.loc[idx, 'trips_am'] = stop_trips['total_trips_am'].sum()
            qualifying_stops_gdf.loc[idx, 'trips_pm'] = stop_trips['total_trips_pm'].sum()

    logging.info(f"Found {len(qualifying_stops_gdf)} qualifying bus stops.")
    return qualifying_stops_gdf


# Step 3: Bus stops with peak hours
#step3 = bus_stops_peak_hours(step1, mode=mode)

#For bus stops (if any): Merge agency_name into step3
# if step3 is not None:
#     step3 = step3.merge(
#         step1.agency[['agency_id', 'agency_name']],
#         on='agency_id',
#         how='left'
#     )
# else:
#     print("No high-frequency bus stops identified. Skipping agency_name merge for bus stops.")
"""# Step #4: Bus stop intersections

This step finds bus stops that are near each other (within 150 or 500 feet) but serve different bus routes. This prevents stops on the same route
from intersecting, which Cal-ITP believed was against the intent of the law.
"""



def identify_bus_stop_intersections(feed, bus_peak_gdf, mode='maximal'):
    """
    Identifies intersecting bus stops based on a distance threshold and frequency criteria.
    Implements the frequency logic from the supervisor's graphic.
    
    Parameters:
    - feed: GTFS feed containing routes, trips, and stop_times
    - bus_peak_gdf: GeoDataFrame with bus stops meeting peak hour criteria
    - mode: 'minimal' or 'maximal' mode
    
    Returns:
    - GeoDataFrame with qualifying intersecting bus stops
    """
    import logging
    logging.info(f"Identifying bus stop intersections in {mode} mode")
    
    # Set distance threshold based on mode
    if mode == 'minimal':
        distance_threshold_feet = 150
    else:
        distance_threshold_feet = 500
    
    # Convert feet to meters for spatial operations
    distance_threshold_meters = distance_threshold_feet * 0.3048
    
    # Determine key columns
    stop_key = get_key(bus_peak_gdf, ['prefixed_stop_id', 'stop_id'])
    route_key = get_key(feed.routes, ['prefixed_route_id', 'route_id'])
    trip_key = get_key(feed.trips, ['prefixed_trip_id', 'trip_id'])
    agency_key = get_key(feed.routes, ['prefixed_agency_id', 'agency_id'])
    
    if not all([stop_key, route_key, trip_key]):
        logging.error("Missing required key columns")
        logging.info(f"bus_peak_gdf columns: {bus_peak_gdf.columns.tolist()}")
        logging.info(f"feed.routes columns: {feed.routes.columns.tolist()}")
        logging.info(f"feed.trips columns: {feed.trips.columns.tolist()}")
        raise ValueError("Missing required key columns")
    
    # ---- Calculate frequencies for each route at each stop ----
    logging.info("Calculating route frequencies at each stop")
    
    # Define peak periods based on mode
    if mode == 'minimal':
        am_start = 6 * 3600   # 6 AM in seconds
        am_end = 9 * 3600     # 9 AM
        pm_start = 15 * 3600  # 3 PM
        pm_end = 19 * 3600    # 7 PM
    else:
        am_start = 0          # Midnight
        am_end = 12 * 3600    # Noon
        pm_start = 12 * 3600  # Noon
        pm_end = 24 * 3600    # Midnight
    
    # Link trips to routes
    trip_route_map = dict(zip(feed.trips[trip_key], feed.trips[route_key]))
    
    # Process stop times to calculate frequencies
    stop_times = feed.stop_times.copy()
    
    # Ensure stop_times has required columns
    if stop_key not in stop_times.columns and 'stop_id' in stop_times.columns:
        stop_times[stop_key] = stop_times['stop_id']
    
    # Add route information to stop_times
    stop_times['route_id'] = stop_times[trip_key].map(trip_route_map)
    
    # Filter for stops in our dataset
    stop_times = stop_times[stop_times[stop_key].isin(bus_peak_gdf[stop_key])]
    
    # Calculate frequencies per route per stop
    route_frequencies = {}  # (stop_id, route_id) -> {'am': freq, 'pm': freq}
    
    # Process in batches to manage memory
    for stop_id in bus_peak_gdf[stop_key].unique():
        stop_data = stop_times[stop_times[stop_key] == stop_id]
        for route_id in stop_data['route_id'].unique():
            if pd.isna(route_id):
                continue
                
            route_stop_data = stop_data[stop_data['route_id'] == route_id]
            
            # Calculate AM frequency (max trips in any 60-min window)
            am_data = route_stop_data[(route_stop_data['arrival_time'] >= am_start) & 
                                     (route_stop_data['arrival_time'] <= am_end)]
            am_freq = 0
            if not am_data.empty:
                arrival_times = sorted(am_data['arrival_time'].tolist())
                for start_time in arrival_times:
                    end_time = start_time + 3600  # One hour later
                    window_count = sum(1 for t in arrival_times if start_time <= t <= end_time)
                    am_freq = max(am_freq, window_count)
            
            # Calculate PM frequency
            pm_data = route_stop_data[(route_stop_data['arrival_time'] >= pm_start) & 
                                     (route_stop_data['arrival_time'] <= pm_end)]
            pm_freq = 0
            if not pm_data.empty:
                arrival_times = sorted(pm_data['arrival_time'].tolist())
                for start_time in arrival_times:
                    end_time = start_time + 3600  # One hour later
                    window_count = sum(1 for t in arrival_times if start_time <= t <= end_time)
                    pm_freq = max(pm_freq, window_count)
            
            route_frequencies[(stop_id, route_id)] = {'am': am_freq, 'pm': pm_freq}
    
    # ---- Apply frequency thresholds based on the graphic ----
    logging.info("Applying frequency thresholds")
    
    # Define minimum frequency threshold (equivalent to 20-min headway)
    min_freq_threshold = 3  # 3 buses per hour
    
    # Identify qualifying stops based on frequency
    qualifying_stops = set()
    stop_route_info = {}  # store frequency info for output
    
    for stop_id in bus_peak_gdf[stop_key].unique():
        # Get all routes serving this stop
        stop_routes = [r for r in route_frequencies.keys() if r[0] == stop_id]
        
        # Check if it qualifies based on individual route frequencies (for minimal)
        qualifies_minimal = False
        for stop_route in stop_routes:
            route_id = stop_route[1]
            freqs = route_frequencies[stop_route]
            
            if freqs['am'] >= min_freq_threshold and freqs['pm'] >= min_freq_threshold:
                qualifies_minimal = True
                break
        
        # For maximal, routes can be combined
        qualifies_maximal = qualifies_minimal  # If it qualifies for minimal, it also qualifies for maximal
        
        if not qualifies_minimal:
            total_am_freq = sum(route_frequencies.get((stop_id, r[1]), {}).get('am', 0) for r in stop_routes)
            total_pm_freq = sum(route_frequencies.get((stop_id, r[1]), {}).get('pm', 0) for r in stop_routes)
            
            if total_am_freq >= min_freq_threshold and total_pm_freq >= min_freq_threshold:
                qualifies_maximal = True
        
        # Record qualification status
        if (mode == 'minimal' and qualifies_minimal) or (mode == 'maximal' and qualifies_maximal):
            qualifying_stops.add(stop_id)
            
            # Store frequency info for this stop
            if mode == 'minimal':
                # For minimal, use the max frequency of any qualifying route
                max_am = max([route_frequencies.get((stop_id, r[1]), {}).get('am', 0) for r in stop_routes])
                max_pm = max([route_frequencies.get((stop_id, r[1]), {}).get('pm', 0) for r in stop_routes])
                stop_route_info[stop_id] = {'am_freq': max_am, 'pm_freq': max_pm}
            else:
                # For maximal, use the sum of all route frequencies
                total_am = sum(route_frequencies.get((stop_id, r[1]), {}).get('am', 0) for r in stop_routes)
                total_pm = sum(route_frequencies.get((stop_id, r[1]), {}).get('pm', 0) for r in stop_routes)
                stop_route_info[stop_id] = {'am_freq': total_am, 'pm_freq': total_pm}
    
    logging.info(f"Found {len(qualifying_stops)} qualifying stops based on frequency")
    
    # Filter to only qualifying stops
    filtered_stops = bus_peak_gdf[bus_peak_gdf[stop_key].isin(qualifying_stops)].copy()
    
    # ---- Find intersections between stops ----
    logging.info("Finding intersections between stops")
    
    # Project to a coordinate system that uses meters
    if filtered_stops.crs != "EPSG:4326":
        filtered_stops = filtered_stops.to_crs(epsg=4326)
    
    # Convert to projected CRS for accurate distance measurement
    filtered_stops_projected = filtered_stops.to_crs(epsg=3857)  # Web Mercator
    
    # Build spatial index
    spatial_index = filtered_stops_projected.sindex
    
    # Find intersections
    intersecting_pairs = []
    
    for idx, stop_a in filtered_stops_projected.iterrows():
        # Create buffer around stop
        buffer = stop_a.geometry.buffer(distance_threshold_meters)
        
        # Find potential intersections
        potential_matches_idx = list(spatial_index.intersection(buffer.bounds))
        potential_matches = filtered_stops_projected.iloc[potential_matches_idx]
        
        for idx_b, stop_b in potential_matches.iterrows():
            # Skip if same stop
            if idx == idx_b:
                continue
                
            # Calculate exact distance
            distance = stop_a.geometry.distance(stop_b.geometry)
            
            if distance <= distance_threshold_meters:
                # Check route differences
                stop_a_routes = [key[1] for key in route_frequencies.keys() if key[0] == stop_a[stop_key]]
                stop_b_routes = [key[1] for key in route_frequencies.keys() if key[0] == stop_b[stop_key]]
                
                # Normalize route IDs to handle bidirectional routes
                def normalize_route(route_id):
                    return ''.join(c for c in str(route_id) if c.isdigit()) or str(route_id)
                
                norm_routes_a = {normalize_route(r) for r in stop_a_routes}
                norm_routes_b = {normalize_route(r) for r in stop_b_routes}
                
                # Check if there are different routes
                has_different_routes = not norm_routes_a.issubset(norm_routes_b) or not norm_routes_b.issubset(norm_routes_a)
                
                if has_different_routes:
                    intersecting_pairs.append((stop_a[stop_key], stop_b[stop_key]))
    
    # Extract unique intersecting stop IDs
    intersecting_stops = set()
    for stop_a, stop_b in intersecting_pairs:
        intersecting_stops.add(stop_a)
        intersecting_stops.add(stop_b)
    
    logging.info(f"Found {len(intersecting_stops)} intersecting stops within {distance_threshold_feet} feet")
    
    # ---- Create output GeoDataFrame ----
    # Filter to intersecting stops only
    intersecting_gdf = filtered_stops[filtered_stops[stop_key].isin(intersecting_stops)].copy()
    
    # Add frequency data to output
    for idx, row in intersecting_gdf.iterrows():
        stop_id = row[stop_key]
        if stop_id in stop_route_info:
            intersecting_gdf.loc[idx, 'am_freq'] = stop_route_info[stop_id]['am_freq']
            intersecting_gdf.loc[idx, 'pm_freq'] = stop_route_info[stop_id]['pm_freq']
    
    # Add qualification field
    intersecting_gdf['qualification'] = f'Bus stop with intersection within {distance_threshold_feet} feet'
    
    # Ensure CRS is 4326 for output
    if intersecting_gdf.crs != "EPSG:4326":
        intersecting_gdf = intersecting_gdf.to_crs(epsg=4326)
    
    logging.info(f"Identified {len(intersecting_gdf)} bus stop intersections in {mode} mode")
    return intersecting_gdf 
# Run the function with the updated code
# if step3 is not None:
#     step4 = identify_bus_stop_intersections(step1, step3, mode=mode)
# else:
#     print("Skipping intersection analysis due to no qualifying bus stops.")
#     step4 = gpd.GeoDataFrame()


def validate_and_fix_maximal_minimal_consistency(minimal_stops, maximal_stops, stop_key='prefixed_stop_id'):
    """
    Ensures maximal stops always include all minimal stops.
    """
    # Get the stop IDs from each set
    minimal_stop_ids = set(minimal_stops[stop_key])
    maximal_stop_ids = set(maximal_stops[stop_key])
    
    # Find stops that are in minimal but not maximal (anomalies)
    missing_from_maximal = minimal_stop_ids - maximal_stop_ids
    
    if missing_from_maximal:
        print(f"Found {len(missing_from_maximal)} stops in minimal that are missing from maximal - fixing inconsistency")
        
        # Get the missing stops from the minimal set
        missing_stops = minimal_stops[minimal_stops[stop_key].isin(missing_from_maximal)]
        
        # Add them to the maximal set
        maximal_stops = pd.concat([maximal_stops, missing_stops], ignore_index=True)
    
    return maximal_stops



"""# Step #5: Merge datasets

This step combines the rail/ferry stops and high-quality bus stops (from steps 2 and 4) into one dataset. It then exports this dataset as a shapefile of the stops (without 1/2 mile buffers) to a designated file path.
"""

def merge_transit_stops(rail_ferry_brt_gdf, bus_stops_gdf, mode='maximal',
                        output_path=r"C:\Users\marce\OneDrive\Documents\UCLA ITS\Transit Zoning Project\Transit Job Output"):
    """
    Merges rail/ferry/BRT stops with high-frequency bus stops, ensuring CRS consistency and handling missing data.

    Params:
        rail_ferry_brt_gdf: GeoDataFrame of rail, ferry, and BRT stops.
        bus_stops_gdf: GeoDataFrame of high-frequency bus stops.
        mode: 'minimal' or 'maximal' mode for transit stop filtering.
        output_path: Directory where the final shapefile is saved.

    Returns:
        GeoDataFrame with merged high-quality transit stops.
    """
    # Ensure CRS is set correctly
    if rail_ferry_brt_gdf.crs is None:
        rail_ferry_brt_gdf.set_crs(epsg=4326, inplace=True)

    # Assign qualification labels
    rail_ferry_brt_gdf['qualification'] = 'Rail, Ferry, BRT stop'

    if bus_stops_gdf is None or bus_stops_gdf.empty:
        print("⚠️ No qualifying bus stops found. Proceeding with only rail/ferry/BRT stops.")
        combined_gdf = rail_ferry_brt_gdf.copy()
    else:
        # Ensure CRS matches before merging
        if bus_stops_gdf.crs is None:
            bus_stops_gdf.set_crs(epsg=4326, inplace=True)

        if rail_ferry_brt_gdf.crs != bus_stops_gdf.crs:
            bus_stops_gdf = bus_stops_gdf.to_crs(rail_ferry_brt_gdf.crs)

        bus_stops_gdf['qualification'] = 'High-Frequency Bus Stop'

        # Concatenate data
        combined_gdf = gpd.GeoDataFrame(
            pd.concat([rail_ferry_brt_gdf, bus_stops_gdf], ignore_index=True),
            geometry='geometry',
            crs=rail_ferry_brt_gdf.crs
        )

    # Define necessary columns
    necessary_columns = [
        'prefixed_stop_id', 'stop_name', 'stop_lat', 'stop_lon', 'geometry',
        'agency_name', 'headway_secs', 'trips_am', 'trips_pm', 'qualification'
    ]

    # Ensure required columns exist
    for col in necessary_columns:
        if col not in combined_gdf.columns:
            combined_gdf[col] = None  

    # Aggregate data to remove duplicates
    aggregation_functions = {
        'qualification': lambda x: '; '.join(set(x.dropna())),
        'geometry': 'first',
        'stop_lat': 'first',
        'stop_lon': 'first',
        'stop_name': 'first',
        'agency_name': lambda x: '; '.join(set(x.dropna())),
        'headway_secs': 'mean',
        'trips_am': 'sum',
        'trips_pm': 'sum'
    }

    all_stops_aggregated = combined_gdf.groupby('prefixed_stop_id', as_index=False).agg(aggregation_functions)

    # Convert to GeoDataFrame
    all_stops_aggregated_gdf = gpd.GeoDataFrame(
        all_stops_aggregated,
        geometry='geometry',
        crs=rail_ferry_brt_gdf.crs
    )

    # Shorten column names for Shapefile compatibility
    column_renaming = {
        'prefixed_stop_id': 'stop_id',
        'qualification': 'qualificat',
        'agency_name': 'agency_nm',
        'headway_secs': 'headway_s',
        'trips_am': 'trips_am',
        'trips_pm': 'trips_pm'
    }
    all_stops_aggregated_gdf.rename(columns=column_renaming, inplace=True)

    # Ensure output directory exists
    if not os.path.exists(output_path):
        os.makedirs(output_path)
        print(f"✅ Created directory: {output_path}")

    # Save the shapefile
    output_file = os.path.join(output_path, "high_quality_stops.shp")
    try:
        all_stops_aggregated_gdf.to_file(output_file, driver="ESRI Shapefile")
        print(f"✅ Saved shapefile: {output_file}")
    except Exception as e:
        print(f"❌ Error saving shapefile: {e}")

    return all_stops_aggregated_gdf

# Run the function with the updated code
#step5 = merge_transit_stops(step2, step4, mode='maximal')
"""# Step #6: Buffer transit stops

This step creates a buffer zone (1/2 mile) around each of the transit stops. These are the high-quality transit areas resulting from
the stops. This step exports the buffer zone to a designated file path.
"""

def buffer_transit_stops(high_quality_stops_gdf, buffer_distance_miles=0.5,
                         output_path=r"C:\Users\marce\OneDrive\Documents\UCLA ITS\Transit Zoning Project\Transit Job Output"):
    """
    Creates a buffer zone (1/2 mile) around each transit stop and exports it as a shapefile.

    Params:
        high_quality_stops_gdf: GeoDataFrame containing high-quality transit stops.
        buffer_distance_miles: Buffer radius in miles (default 0.5 miles).
        output_path: Directory where the buffer shapefile is saved.

    Returns:
        GeoDataFrame with buffered transit stops.
    """
    # Convert buffer distance to meters
    buffer_distance_meters = buffer_distance_miles * 1609.34  

    # Ensure CRS for accurate buffering
    if high_quality_stops_gdf.crs.to_epsg() != 32611:  
        high_quality_stops_gdf = high_quality_stops_gdf.to_crs(epsg=32611)

    # Apply buffer
    high_quality_stops_gdf['buffer'] = high_quality_stops_gdf.geometry.buffer(buffer_distance_meters)
    buffered_stops_gdf = high_quality_stops_gdf.set_geometry('buffer').copy()

    # Drop original geometry to avoid conflicts
    buffered_stops_gdf = buffered_stops_gdf.drop(columns='geometry')

    # Save to shapefile
    output_file = os.path.join(output_path, "file_buffer2.shp")
    try:
        buffered_stops_gdf.to_file(output_file, driver="ESRI Shapefile")
        print(f"✅ Saved shapefile: {output_file}")
    except Exception as e:
        print(f"❌ Error saving shapefile: {e}")

    return buffered_stops_gdf
#step6 = buffer_transit_stops(step5)

def run_transit_zoning_pipeline(gtfs_paths, output_base_path):
    """
    Run the complete transit zoning pipeline for both minimal and maximal definitions.
    
    Parameters:
    - gtfs_paths: List of paths to GTFS files
    - output_base_path: Base directory for output files
    
    Returns:
    - Dictionary containing both minimal and maximal results
    """
    
    
    # Configure logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        filename=os.path.join(output_base_path, 'transit_zoning.log'),
        filemode='w'
    )
    
    console = logging.StreamHandler()
    console.setLevel(logging.INFO)
    logging.getLogger('').addHandler(console)
    
    logging.info("Starting Transit Zoning Pipeline")
    
    # Step 1: Load and combine GTFS data (same for both modes)
    logging.info("Step 1: Loading and combining GTFS data")
    step1 = load_and_combine_gtfs(gtfs_paths)
    
    # Step 2: Identify rail, ferry, and BRT stops (using maximal first)
    logging.info("Step 2: Identifying rail, ferry, and BRT stops (maximal)")
    step2_max = rail_ferry_brt(step1, mode='maximal')
    
    # Step 3-6: Process maximal definition
    logging.info("Processing maximal definition")
    max_mode = 'maximal'
    output_path_max = os.path.join(output_base_path, "Maximal")
    os.makedirs(output_path_max, exist_ok=True)
    
    step3_max = bus_stops_peak_hours(step1, mode=max_mode)
    step4_max = identify_bus_stop_intersections(step1, step3_max, mode=max_mode)
    step5_max = merge_transit_stops(step2_max, step4_max, mode=max_mode, output_path=output_path_max)
    step6_max = buffer_transit_stops(step5_max, buffer_distance_miles=0.5, output_path=output_path_max)
    
    # Step 2-6: Process minimal definition
    logging.info("Processing minimal definition")
    min_mode = 'minimal'
    output_path_min = os.path.join(output_base_path, "Minimal")
    os.makedirs(output_path_min, exist_ok=True)
    
    step2_min = rail_ferry_brt(step1, mode=min_mode)
    step3_min = bus_stops_peak_hours(step1, mode=min_mode)
    step4_min = identify_bus_stop_intersections(step1, step3_min, mode=min_mode)
    
    # Ensure consistency between minimal and maximal
    logging.info("Validating consistency between minimal and maximal definitions")
    stop_key = get_key(step4_min, ['prefixed_stop_id', 'stop_id'])
    step4_max = validate_and_fix_maximal_minimal_consistency(step4_min, step4_max, stop_key=stop_key)
    
    step5_min = merge_transit_stops(step2_min, step4_min, mode=min_mode, output_path=output_path_min)
    step6_min = buffer_transit_stops(step5_min, buffer_distance_miles=0.5, output_path=output_path_min)
    
    logging.info("Transit Zoning Pipeline completed successfully")
    
    return {
        'minimal': {
            'rail_ferry_brt': step2_min,
            'bus_peaks': step3_min,
            'bus_intersections': step4_min,
            'merged': step5_min,
            'buffered': step6_min
        },
        'maximal': {
            'rail_ferry_brt': step2_max,
            'bus_peaks': step3_max,
            'bus_intersections': step4_max,
            'merged': step5_max,
            'buffered': step6_max
        }
    }

# Change this to your output path
output_path = r"C:\Users\marce\OneDrive\Documents\UCLA ITS\Transit Zoning Project\Transit Job Output"

# Run the complete pipeline
results = run_transit_zoning_pipeline(gtfs_path_2024, output_path)